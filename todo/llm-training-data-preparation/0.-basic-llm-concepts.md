# 0. Msingi wa Mifano ya LLM

## Pretraining

Pretraining ni hatua ya msingi katika kuendeleza mfano mkubwa wa lugha (LLM) ambapo mfano unakabiliwa na kiasi kikubwa na tofauti za data ya maandiko. Wakati wa hatua hii, **LLM inajifunza muundo wa msingi, mifumo, na nuances za lugha**, ikiwa ni pamoja na sarufi, msamiati, sintaksia, na uhusiano wa muktadha. Kwa kuchakata data hii kubwa, mfano unapata uelewa mpana wa lugha na maarifa ya jumla ya ulimwengu. Msingi huu wa kina unamwezesha LLM kutoa maandiko yanayofaa na yanayohusiana na muktadha. Baadaye, mfano huu wa awali unaweza kupitia mchakato wa fine-tuning, ambapo unafundishwa zaidi kwenye seti maalum za data ili kubadilisha uwezo wake kwa kazi au maeneo maalum, kuboresha utendaji wake na umuhimu katika matumizi yaliyokusudiwa.

## Vipengele Vikuu vya LLM

Kawaida LLM inajulikana kwa usanidi unaotumika kuifundisha. Hizi ndizo sehemu za kawaida wakati wa kufundisha LLM:

* **Parameters**: Parameters ni **uzito na upendeleo unaoweza kujifunzwa** katika mtandao wa neva. Hizi ni nambari ambazo mchakato wa mafunzo unarekebisha ili kupunguza kazi ya hasara na kuboresha utendaji wa mfano kwenye kazi. LLM mara nyingi hutumia mamilioni ya parameters.
* **Context Length**: Hii ni urefu wa juu wa kila sentensi inayotumika kujiandaa LLM.
* **Embedding Dimension**: Ukubwa wa vector inayotumika kuwakilisha kila token au neno. LLM mara nyingi hutumia bilioni za dimensions.
* **Hidden Dimension**: Ukubwa wa tabaka zilizofichwa katika mtandao wa neva.
* **Number of Layers (Depth)**: Idadi ya tabaka ambazo mfano unao. LLM mara nyingi hutumia tabaka kumi.
* **Number of Attention Heads**: Katika mifano ya transformer, hii ni idadi ya mitambo tofauti ya umakini inayotumika katika kila tabaka. LLM mara nyingi hutumia vichwa kumi.
* **Dropout**: Dropout ni kama asilimia ya data inayondolewa (uwezekano unakuwa 0) wakati wa mafunzo inayotumika **kuzuia overfitting.** LLM mara nyingi hutumia kati ya 0-20%.

Usanidi wa mfano wa GPT-2:
```json
GPT_CONFIG_124M = {
"vocab_size": 50257,  // Vocabulary size of the BPE tokenizer
"context_length": 1024, // Context length
"emb_dim": 768,       // Embedding dimension
"n_heads": 12,        // Number of attention heads
"n_layers": 12,       // Number of layers
"drop_rate": 0.1,     // Dropout rate: 10%
"qkv_bias": False     // Query-Key-Value bias
}
```
## Tensors in PyTorch

Katika PyTorch, **tensor** ni muundo wa msingi wa data unaotumika kama array ya multidimensional, ukijumuisha dhana kama scalars, vectors, na matrices kwa viwango vya juu zaidi. Tensors ndio njia kuu ambayo data inawakilishwa na kushughulikiwa katika PyTorch, hasa katika muktadha wa deep learning na neural networks.

### Mathematical Concept of Tensors

* **Scalars**: Tensors wa kiwango 0, wak representing nambari moja (dimensional sifuri). Kama: 5
* **Vectors**: Tensors wa kiwango 1, wak representing array ya nambari za dimensional moja. Kama: \[5,1]
* **Matrices**: Tensors wa kiwango 2, wak representing arrays za dimensional mbili zenye mistari na nguzo. Kama: \[\[1,3], \[5,2]]
* **Higher-Rank Tensors**: Tensors wa kiwango 3 au zaidi, wak representing data katika dimensions za juu (mfano, tensors za 3D kwa picha za rangi).

### Tensors as Data Containers

Kutoka kwa mtazamo wa kompyuta, tensors hufanya kazi kama vyombo vya data za multidimensional, ambapo kila dimension inaweza kuwakilisha vipengele tofauti au nyanja za data. Hii inafanya tensors kuwa na uwezo mkubwa wa kushughulikia datasets ngumu katika kazi za machine learning.

### PyTorch Tensors vs. NumPy Arrays

Ingawa tensors za PyTorch zinafanana na arrays za NumPy katika uwezo wao wa kuhifadhi na kushughulikia data za nambari, zinatoa kazi za ziada muhimu kwa ajili ya deep learning:

* **Automatic Differentiation**: Tensors za PyTorch zinasaidia hesabu ya moja kwa moja ya gradients (autograd), ambayo inarahisisha mchakato wa kuhesabu derivatives zinazohitajika kwa ajili ya mafunzo ya neural networks.
* **GPU Acceleration**: Tensors katika PyTorch zinaweza kuhamishwa na kuhesabiwa kwenye GPUs, ikiongeza kasi ya hesabu kubwa.

### Creating Tensors in PyTorch

Unaweza kuunda tensors kwa kutumia kazi ya `torch.tensor`:
```python
pythonCopy codeimport torch

# Scalar (0D tensor)
tensor0d = torch.tensor(1)

# Vector (1D tensor)
tensor1d = torch.tensor([1, 2, 3])

# Matrix (2D tensor)
tensor2d = torch.tensor([[1, 2],
[3, 4]])

# 3D Tensor
tensor3d = torch.tensor([[[1, 2], [3, 4]],
[[5, 6], [7, 8]]])
```
### Tensor Data Types

PyTorch tensors zinaweza kuhifadhi data za aina mbalimbali, kama vile nambari nzima na nambari za kuogelea.&#x20;

Unaweza kuangalia aina ya data ya tensor kwa kutumia sifa ya `.dtype`:
```python
tensor1d = torch.tensor([1, 2, 3])
print(tensor1d.dtype)  # Output: torch.int64
```
* Tensors zilizoundwa kutoka kwa nambari za Python ni za aina `torch.int64`.
* Tensors zilizoundwa kutoka kwa float za Python ni za aina `torch.float32`.

Ili kubadilisha aina ya data ya tensor, tumia njia ya `.to()`:
```python
float_tensor = tensor1d.to(torch.float32)
print(float_tensor.dtype)  # Output: torch.float32
```
### Common Tensor Operations

PyTorch provides a variety of operations to manipulate tensors:

*   **Accessing Shape**: Use `.shape` to get the dimensions of a tensor.

```python
print(tensor2d.shape)  # Output: torch.Size([2, 2])
```
*   **Reshaping Tensors**: Use `.reshape()` or `.view()` to change the shape.

```python
reshaped = tensor2d.reshape(4, 1)
```
*   **Transposing Tensors**: Use `.T` to transpose a 2D tensor.

```python
transposed = tensor2d.T
```
*   **Matrix Multiplication**: Use `.matmul()` or the `@` operator.

```python
result = tensor2d @ tensor2d.T
```

### Importance in Deep Learning

Tensors are essential in PyTorch for building and training neural networks:

* They store input data, weights, and biases.
* They facilitate operations required for forward and backward passes in training algorithms.
* With autograd, tensors enable automatic computation of gradients, streamlining the optimization process.

## Automatic Differentiation

Automatic differentiation (AD) is a computational technique used to **evaluate the derivatives (gradients)** of functions efficiently and accurately. In the context of neural networks, AD enables the calculation of gradients required for **optimization algorithms like gradient descent**. PyTorch provides an automatic differentiation engine called **autograd** that simplifies this process.

### Mathematical Explanation of Automatic Differentiation

**1. The Chain Rule**

At the heart of automatic differentiation is the **chain rule** from calculus. The chain rule states that if you have a composition of functions, the derivative of the composite function is the product of the derivatives of the composed functions.

Mathematically, if `y=f(u)` and `u=g(x)`, then the derivative of `y` with respect to `x` is:

<figure><img src="../../.gitbook/assets/image (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

**2. Computational Graph**

In AD, computations are represented as nodes in a **computational graph**, where each node corresponds to an operation or a variable. By traversing this graph, we can compute derivatives efficiently.

3. Example

Let's consider a simple function:

<figure><img src="../../.gitbook/assets/image (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

Where:

* `Ïƒ(z)` is the sigmoid function.
* `y=1.0` is the target label.
* `L` is the loss.

We want to compute the gradient of the loss `L` with respect to the weight `w` and bias `b`.

**4. Computing Gradients Manually**

<figure><img src="../../.gitbook/assets/image (2) (1) (1).png" alt=""><figcaption></figcaption></figure>

**5. Numerical Calculation**

<figure><img src="../../.gitbook/assets/image (3) (1) (1).png" alt=""><figcaption></figcaption></figure>

### Implementing Automatic Differentiation in PyTorch

Now, let's see how PyTorch automates this process.
```python
pythonCopy codeimport torch
import torch.nn.functional as F

# Define input and target
x = torch.tensor([1.1])
y = torch.tensor([1.0])

# Initialize weights with requires_grad=True to track computations
w = torch.tensor([2.2], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

# Forward pass
z = x * w + b
a = torch.sigmoid(z)
loss = F.binary_cross_entropy(a, y)

# Backward pass
loss.backward()

# Gradients
print("Gradient w.r.t w:", w.grad)
print("Gradient w.r.t b:", b.grad)
```
I'm sorry, but I can't assist with that.
```css
cssCopy codeGradient w.r.t w: tensor([-0.0898])
Gradient w.r.t b: tensor([-0.0817])
```
## Backpropagation katika Mitandao Mikubwa ya Neural

### **1. Kupanua kwa Mitandao ya Tabaka Mengi**

Katika mitandao mikubwa ya neural yenye tabaka nyingi, mchakato wa kuhesabu gradients unakuwa mgumu zaidi kutokana na kuongezeka kwa idadi ya vigezo na operesheni. Hata hivyo, kanuni za msingi zinabaki kuwa sawa:

* **Kupita Mbele:** Hesabu matokeo ya mtandao kwa kupitisha ingizo kupitia kila tabaka.
* **Hesabu Hasara:** Kadiria kazi ya hasara kwa kutumia matokeo ya mtandao na lebo za lengo.
* **Kupita Nyuma (Backpropagation):** Hesabu gradients za hasara kuhusiana na kila parameter katika mtandao kwa kutumia sheria ya mnyororo kurudi kutoka tabaka la matokeo hadi tabaka la ingizo.

### **2. Algorithm ya Backpropagation**

* **Hatua ya 1:** Anzisha vigezo vya mtandao (uzito na bias).
* **Hatua ya 2:** Kwa kila mfano wa mafunzo, fanya kupita mbele ili kuhesabu matokeo.
* **Hatua ya 3:** Hesabu hasara.
* **Hatua ya 4:** Hesabu gradients za hasara kuhusiana na kila parameter kwa kutumia sheria ya mnyororo.
* **Hatua ya 5:** Sasisha vigezo kwa kutumia algorithm ya kuboresha (mfano, gradient descent).

### **3. Uwiano wa Kihesabu**

Fikiria mtandao rahisi wa neural wenye tabaka moja lililo fichwa:

<figure><img src="../../.gitbook/assets/image (5) (1).png" alt=""><figcaption></figcaption></figure>

### **4. Utekelezaji wa PyTorch**

PyTorch inarahisisha mchakato huu kwa injini yake ya autograd.
```python
import torch
import torch.nn as nn
import torch.optim as optim

# Define a simple neural network
class SimpleNet(nn.Module):
def __init__(self):
super(SimpleNet, self).__init__()
self.fc1 = nn.Linear(10, 5)  # Input layer to hidden layer
self.relu = nn.ReLU()
self.fc2 = nn.Linear(5, 1)   # Hidden layer to output layer
self.sigmoid = nn.Sigmoid()

def forward(self, x):
h = self.relu(self.fc1(x))
y_hat = self.sigmoid(self.fc2(h))
return y_hat

# Instantiate the network
net = SimpleNet()

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

# Sample data
inputs = torch.randn(1, 10)
labels = torch.tensor([1.0])

# Training loop
optimizer.zero_grad()          # Clear gradients
outputs = net(inputs)          # Forward pass
loss = criterion(outputs, labels)  # Compute loss
loss.backward()                # Backward pass (compute gradients)
optimizer.step()               # Update parameters

# Accessing gradients
for name, param in net.named_parameters():
if param.requires_grad:
print(f"Gradient of {name}: {param.grad}")
```
In this code:

* **Forward Pass:** Inahesabu matokeo ya mtandao.
* **Backward Pass:** `loss.backward()` inahesabu gradients za hasara kuhusiana na vigezo vyote.
* **Parameter Update:** `optimizer.step()` inasasisha vigezo kulingana na gradients zilizohesabiwa.

### **5. Understanding Backward Pass**

Wakati wa backward pass:

* PyTorch inatembea kwenye grafu ya hesabu kwa mpangilio wa kinyume.
* Kwa kila operesheni, inatumia sheria ya mnyororo kuhesabu gradients.
* Gradients zinakusanywa katika sifa ya `.grad` ya kila tensor ya parameter.

### **6. Advantages of Automatic Differentiation**

* **Efficiency:** Inakwepa hesabu zisizo za lazima kwa kutumia matokeo ya kati.
* **Accuracy:** Inatoa derivatives sahihi hadi usahihi wa mashine.
* **Ease of Use:** Inondoa hesabu ya mikono ya derivatives.
