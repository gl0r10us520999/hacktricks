# 7.2. Dostosowywanie do przestrzegania instrukcji

{% hint style="success" %}
Celem tej sekcji jest pokazanie, jak **dostosować już wstępnie wytrenowany model do przestrzegania instrukcji**, a nie tylko generowania tekstu, na przykład, odpowiadając na zadania jako chatbot.
{% endhint %}

## Zbiór danych

Aby dostosować LLM do przestrzegania instrukcji, potrzebny jest zbiór danych z instrukcjami i odpowiedziami, aby dostosować LLM. Istnieją różne formaty do trenowania LLM w celu przestrzegania instrukcji, na przykład:

* Przykład stylu promptu Apply Alpaca:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
* Przykład stylu promptu Phi-3:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Szkolenie LLM z tego rodzaju zestawami danych zamiast tylko surowego tekstu pomaga LLM zrozumieć, że musi udzielać konkretnych odpowiedzi na zadawane pytania.

Dlatego jedną z pierwszych rzeczy do zrobienia z zestawem danych, który zawiera prośby i odpowiedzi, jest sformatowanie tych danych w pożądanym formacie promptu, na przykład:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Then, as always, it's needed to separate the dataset in sets for training, validation and testing.

## Batching & Data Loaders

Then, it's needed to batch all the inputs and expected outputs for the training. For this, it's needed to:

* Tokenize the texts
* Pad all the samples to the same length (usually the length will be as big as the context length used to pre-train the LLM)
* Create the expected tokens by shifting 1 the input in a custom collate function
* Replace some padding tokens with -100 to exclude them from the training loss: After the first `endoftext` token, substitute all the other `endoftext` tokens by -100 (because using `cross_entropy(...,ignore_index=-100)` means that it'll ignore targets with -100)
* \[Optional] Mask using -100 also all the tokens belonging to the question so the LLM learns only how to generate the answer. In the Apply Alpaca style this will mean to mask everything until `### Response:`

With this created, it's time to crate the data loaders for each dataset (training, validation and test).

## Load pre-trained LLM & Fine tune & Loss Checking

It's needed to load a pre-trained LLM to fine tune it. This was already discussed in other pages. Then, it's possible to use the previously used training function to fine tune the LLM.

During the training it's also possible to see how the training loss and validation loss varies during the epochs to see if the loss is getting reduced and if overfitting is ocurring.\
Remember that overfitting occurs when the training loss is getting reduced but the validation loss is not being reduced or even increasing. To avoid this, the simplest thing to do is to stop the training at the epoch where this behaviour start.

## Response Quality

As this is not a classification fine-tune were it's possible to trust more the loss variations, it's also important to check the quality of the responses in the testing set. Therefore, it's recommended to gather the generated responses from all the testing sets and **check their quality manually** to see if there are wrong answers (note that it's possible for the LLM to create correctly the format and syntax of the response sentence but gives a completely wrong response. The loss variation won't reflect this behaviour).\
Note that it's also possible to perform this review by passing the generated responses and the expected responses to **other LLMs and ask them to evaluate the responses**.

Other test to run to verify the quality of the responses:

1. **Measuring Massive Multitask Language Understanding (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU ocenia wiedzę modelu i umiejętności rozwiązywania problemów w 57 przedmiotach, w tym w naukach humanistycznych, naukach ścisłych i innych. Używa pytań wielokrotnego wyboru do oceny zrozumienia na różnych poziomach trudności, od podstawowego do zaawansowanego zawodowego.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): Ta platforma pozwala użytkownikom porównywać odpowiedzi różnych chatbotów obok siebie. Użytkownicy wprowadzają zapytanie, a wiele chatbotów generuje odpowiedzi, które można bezpośrednio porównać.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca\_eval)**:** AlpacaEval to zautomatyzowany framework oceny, w którym zaawansowany LLM, taki jak GPT-4, ocenia odpowiedzi innych modeli na różne zapytania.
4. **General Language Understanding Evaluation (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE to zbiór dziewięciu zadań z zakresu rozumienia języka naturalnego, w tym analizy sentymentu, implikacji tekstowej i odpowiadania na pytania.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** Na bazie GLUE, SuperGLUE zawiera bardziej wymagające zadania zaprojektowane tak, aby były trudne dla obecnych modeli.
6. **Beyond the Imitation Game Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench to duży benchmark z ponad 200 zadaniami, które testują umiejętności modelu w obszarach takich jak rozumowanie, tłumaczenie i odpowiadanie na pytania.
7. **Holistic Evaluation of Language Models (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM zapewnia kompleksową ocenę w różnych metrykach, takich jak dokładność, odporność i sprawiedliwość.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** Otwarty framework oceny stworzony przez OpenAI, który pozwala na testowanie modeli AI na niestandardowych i standardowych zadaniach.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** Zbiór problemów programistycznych używanych do oceny zdolności generowania kodu przez modele językowe.
10. **Stanford Question Answering Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD składa się z pytań dotyczących artykułów Wikipedii, w których modele muszą zrozumieć tekst, aby odpowiedzieć poprawnie.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** Duży zbiór danych z pytaniami i odpowiedziami trivia, wraz z dokumentami dowodowymi.

and many many more

## Follow instructions fine-tuning code

You can find an example of the code to perform this fine tuning in [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/gpt\_instruction\_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/gpt\_instruction\_finetuning.py)

## References

* [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)
