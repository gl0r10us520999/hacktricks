# 7.2. 指示に従うためのファインチューニング

{% hint style="success" %}
このセクションの目的は、**テキストを生成するだけでなく、指示に従うように既に事前トレーニングされたモデルをファインチューニングする方法**を示すことです。例えば、チャットボットとしてタスクに応答することです。
{% endhint %}

## データセット

指示に従うようにLLMをファインチューニングするためには、指示と応答を含むデータセットが必要です。指示に従うようにLLMをトレーニングするための異なるフォーマットがあります。例えば：

* Apply Alpacaプロンプトスタイルの例：
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
* Phi-3 プロンプトスタイルの例:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
トレーニングデータセットを生のテキストだけでなく、このようなデータセットでLLMをトレーニングすることで、LLMは受け取る質問に対して具体的な応答をする必要があることを理解します。

したがって、リクエストと回答を含むデータセットで最初に行うべきことの1つは、そのデータを希望するプロンプト形式にモデル化することです。例えば：
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Then, as always, it's needed to separate the dataset in sets for training, validation and testing.

## Batching & Data Loaders

Then, it's needed to batch all the inputs and expected outputs for the training. For this, it's needed to:

* テキストをトークン化する
* すべてのサンプルを同じ長さにパディングする（通常、長さはLLMの事前トレーニングに使用されるコンテキストの長さと同じくらい大きくなる）
* カスタムコレート関数で入力を1つシフトして期待されるトークンを作成する
* トレーニング損失から除外するために、いくつかのパディングトークンを-100に置き換える：最初の`endoftext`トークンの後、他のすべての`endoftext`トークンを-100に置き換える（`cross_entropy(...,ignore_index=-100)`を使用することは、-100のターゲットを無視することを意味する）
* \[オプション\] LLMが回答を生成する方法だけを学ぶように、質問に属するすべてのトークンを-100でマスクする。Apply Alpacaスタイルでは、`### Response:`までのすべてをマスクすることを意味する

これが作成されたら、各データセット（トレーニング、検証、テスト）のデータローダーを作成する時が来た。

## Load pre-trained LLM & Fine tune & Loss Checking

事前にトレーニングされたLLMをロードして微調整する必要がある。これは他のページで既に議論されている。次に、以前に使用したトレーニング関数を使用してLLMを微調整することができる。

トレーニング中、エポックの間にトレーニング損失と検証損失がどのように変化するかを見ることもでき、損失が減少しているか、過剰適合が発生しているかを確認できる。\
過剰適合は、トレーニング損失が減少しているが、検証損失が減少していないか、さらには増加している場合に発生することを覚えておいてください。これを避けるために、最も簡単な方法は、この挙動が始まるエポックでトレーニングを停止することです。

## Response Quality

これは分類の微調整ではなく、損失の変動をより信頼できるため、テストセットの応答の質を確認することも重要です。したがって、すべてのテストセットから生成された応答を集めて、**その質を手動で確認する**ことをお勧めします。間違った回答があるかどうかを確認してください（LLMが応答文の形式と構文を正しく作成することは可能ですが、完全に間違った応答を提供することがあります。損失の変動はこの挙動を反映しません）。\
生成された応答と期待される応答を**他のLLMに渡して応答を評価するように依頼する**ことでも、このレビューを実行することが可能です。

応答の質を確認するために実行する他のテスト：

1. **大規模マルチタスク言語理解（**[**MMLU**](https://arxiv.org/abs/2009.03300)**）：** MMLUは、ヒューマニティーズ、サイエンスなど57の科目にわたるモデルの知識と問題解決能力を評価します。さまざまな難易度レベルの理解を評価するために選択肢形式の質問を使用します。
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org)：このプラットフォームでは、異なるチャットボットの応答を並べて比較できます。ユーザーはプロンプトを入力し、複数のチャットボットが生成した応答を直接比較できます。
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**：** AlpacaEvalは、自動評価フレームワークで、GPT-4のような高度なLLMがさまざまなプロンプトに対する他のモデルの応答を評価します。
4. **一般的な言語理解評価（**[**GLUE**](https://gluebenchmark.com/)**）：** GLUEは、感情分析、テキストの含意、質問応答など、9つの自然言語理解タスクのコレクションです。
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**：** GLUEを基にして、SuperGLUEは現在のモデルにとって難しいとされるより挑戦的なタスクを含んでいます。
6. **模倣ゲームベンチマークを超えて（**[**BIG-bench**](https://github.com/google/BIG-bench)**）：** BIG-benchは、推論、翻訳、質問応答などの分野でモデルの能力をテストする200以上のタスクを持つ大規模なベンチマークです。
7. **言語モデルの包括的評価（**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**）：** HELMは、精度、堅牢性、公平性など、さまざまな指標にわたる包括的な評価を提供します。
8. [**OpenAI Evals**](https://github.com/openai/evals)**：** OpenAIによるオープンソースの評価フレームワークで、カスタムおよび標準化されたタスクでAIモデルをテストできます。
9. [**HumanEval**](https://github.com/openai/human-eval)**：** プログラミング問題のコレクションで、言語モデルのコード生成能力を評価するために使用されます。
10. **スタンフォード質問応答データセット（**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**）：** SQuADは、Wikipediaの記事に関する質問で構成されており、モデルは正確に回答するためにテキストを理解する必要があります。
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**：** トリビアの質問と回答の大規模データセットで、証拠文書も含まれています。

and many many more

## Follow instructions fine-tuning code

You can find an example of the code to perform this fine tuning in [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/gpt\_instruction\_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/gpt\_instruction\_finetuning.py)

## References

* [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)
