# 7.2. Fine-Tuning per seguire le istruzioni

{% hint style="success" %}
L'obiettivo di questa sezione è mostrare come **affinare un modello già pre-addestrato per seguire le istruzioni** piuttosto che generare semplicemente testo, ad esempio, rispondendo a compiti come un chatbot.
{% endhint %}

## Dataset

Per affinare un LLM per seguire le istruzioni è necessario avere un dataset con istruzioni e risposte per affinare l'LLM. Ci sono diversi formati per addestrare un LLM a seguire le istruzioni, ad esempio:

* L'esempio di stile prompt Apply Alpaca:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
* Esempio di Stile Prompt Phi-3:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Addestrare un LLM con questo tipo di set di dati invece di semplicemente testo grezzo aiuta il LLM a capire che deve fornire risposte specifiche alle domande che riceve.

Pertanto, una delle prime cose da fare con un set di dati che contiene richieste e risposte è modellare quella data nel formato di prompt desiderato, come:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Then, as always, it's needed to separate the dataset in sets for training, validation and testing.

## Batching & Data Loaders

Then, it's needed to batch all the inputs and expected outputs for the training. For this, it's needed to:

* Tokenizzare i testi
* Aggiungere padding a tutti i campioni alla stessa lunghezza (di solito la lunghezza sarà grande quanto la lunghezza del contesto utilizzato per pre-addestrare il LLM)
* Creare i token attesi spostando di 1 l'input in una funzione di collate personalizzata
* Sostituire alcuni token di padding con -100 per escluderli dalla perdita di addestramento: Dopo il primo token `endoftext`, sostituire tutti gli altri token `endoftext` con -100 (perché usare `cross_entropy(...,ignore_index=-100)` significa che ignorerà i target con -100)
* \[Opzionale] Mascherare usando -100 anche tutti i token appartenenti alla domanda in modo che il LLM impari solo a generare la risposta. Nello stile Apply Alpaca questo significherà mascherare tutto fino a `### Response:`

With this created, it's time to crate the data loaders for each dataset (training, validation and test).

## Load pre-trained LLM & Fine tune & Loss Checking

It's needed to load a pre-trained LLM to fine tune it. This was already discussed in other pages. Then, it's possible to use the previously used training function to fine tune the LLM.

During the training it's also possible to see how the training loss and validation loss varies during the epochs to see if the loss is getting reduced and if overfitting is ocurring.\
Remember that overfitting occurs when the training loss is getting reduced but the validation loss is not being reduced or even increasing. To avoid this, the simplest thing to do is to stop the training at the epoch where this behaviour start.

## Response Quality

As this is not a classification fine-tune were it's possible to trust more the loss variations, it's also important to check the quality of the responses in the testing set. Therefore, it's recommended to gather the generated responses from all the testing sets and **check their quality manually** to see if there are wrong answers (note that it's possible for the LLM to create correctly the format and syntax of the response sentence but gives a completely wrong response. The loss variation won't reflect this behaviour).\
Note that it's also possible to perform this review by passing the generated responses and the expected responses to **other LLMs and ask them to evaluate the responses**.

Other test to run to verify the quality of the responses:

1. **Measuring Massive Multitask Language Understanding (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU valuta la conoscenza e le capacità di problem-solving di un modello in 57 soggetti, comprese le scienze umane, le scienze e altro. Utilizza domande a scelta multipla per valutare la comprensione a vari livelli di difficoltà, dall'elementare all'avanzato professionale.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): Questa piattaforma consente agli utenti di confrontare le risposte di diversi chatbot affiancati. Gli utenti inseriscono un prompt e più chatbot generano risposte che possono essere confrontate direttamente.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca\_eval)**:** AlpacaEval è un framework di valutazione automatizzato in cui un LLM avanzato come GPT-4 valuta le risposte di altri modelli a vari prompt.
4. **General Language Understanding Evaluation (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE è una raccolta di nove compiti di comprensione del linguaggio naturale, tra cui analisi del sentiment, implicazione testuale e risposta a domande.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** Basato su GLUE, SuperGLUE include compiti più impegnativi progettati per essere difficili per i modelli attuali.
6. **Beyond the Imitation Game Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench è un benchmark su larga scala con oltre 200 compiti che testano le capacità di un modello in aree come ragionamento, traduzione e risposta a domande.
7. **Holistic Evaluation of Language Models (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM fornisce una valutazione completa attraverso vari metriche come accuratezza, robustezza e equità.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** Un framework di valutazione open-source di OpenAI che consente di testare modelli AI su compiti personalizzati e standardizzati.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** Una raccolta di problemi di programmazione utilizzati per valutare le capacità di generazione di codice dei modelli di linguaggio.
10. **Stanford Question Answering Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD consiste in domande su articoli di Wikipedia, dove i modelli devono comprendere il testo per rispondere accuratamente.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** Un dataset su larga scala di domande e risposte trivia, insieme a documenti di prova.

and many many more

## Follow instructions fine-tuning code

You can find an example of the code to perform this fine tuning in [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/gpt\_instruction\_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/gpt\_instruction\_finetuning.py)

## References

* [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)
