<details>

<summary><strong>Apprenez le piratage AWS de z√©ro √† h√©ros avec</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (Expert en √©quipe rouge AWS de HackTricks)</strong></a><strong>!</strong></summary>

Autres fa√ßons de soutenir HackTricks :

* Si vous souhaitez voir votre **entreprise annonc√©e dans HackTricks** ou **t√©l√©charger HackTricks en PDF**, consultez les [**PLANS D'ABONNEMENT**](https://github.com/sponsors/carlospolop) !
* Obtenez le [**swag officiel PEASS & HackTricks**](https://peass.creator-spring.com)
* D√©couvrez [**La famille PEASS**](https://opensea.io/collection/the-peass-family), notre collection exclusive de [**NFTs**](https://opensea.io/collection/the-peass-family)
* **Rejoignez le** üí¨ [**groupe Discord**](https://discord.gg/hRep4RUj7f) ou le [**groupe Telegram**](https://t.me/peass) ou **suivez-nous** sur **Twitter** üê¶ [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Partagez vos astuces de piratage en soumettant des PR aux** [**HackTricks**](https://github.com/carlospolop/hacktricks) et [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) d√©p√¥ts GitHub.

</details>


# Types de donn√©es possibles de base

Les donn√©es peuvent √™tre **continues** (valeurs **infinies**) ou **cat√©gorielles** (nominales) o√π la quantit√© de valeurs possibles est **limit√©e**.

## Types cat√©goriels

### Binaire

Juste **2 valeurs possibles** : 1 ou 0. Dans le cas o√π dans un ensemble de donn√©es les valeurs sont au format cha√Æne de caract√®res (par exemple "Vrai" et "Faux"), vous attribuez des nombres √† ces valeurs avec :
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **Ordinal**

Les **valeurs suivent un ordre**, comme dans : 1√®re place, 2√®me place... Si les cat√©gories sont des cha√Ænes de caract√®res (comme : "d√©butant", "amateur", "professionnel", "expert"), vous pouvez les mapper √† des nombres comme nous l'avons vu dans le cas binaire.
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* Pour les **colonnes alphab√©tiques**, vous pouvez les ordonner plus facilement :
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **Cyclique**

Ressemble √† une **valeur ordinale** car il y a un ordre, mais cela ne signifie pas qu'une valeur est plus grande que l'autre. De plus, la **distance entre elles d√©pend de la direction** dans laquelle vous comptez. Exemple : les jours de la semaine, dimanche n'est pas "plus grand" que lundi.

* Il existe **diff√©rentes fa√ßons** d'encoder les caract√©ristiques cycliques, certaines peuvent fonctionner avec seulement quelques algorithmes. **En g√©n√©ral, l'encodage factice peut √™tre utilis√©**.
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **Dates**

Les dates sont des **variables continues**. Elles peuvent √™tre consid√©r√©es comme **cycliques** (car elles se r√©p√®tent) ou comme des **variables ordinales** (car un moment dans le temps est plus grand qu'un pr√©c√©dent).

* Habituellement, les dates sont utilis√©es comme **index**.
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### Multi-cat√©gorie/nominale

**Plus de 2 cat√©gories** sans ordre sp√©cifique. Utilisez `dataset.describe(include='all')` pour obtenir des informations sur les cat√©gories de chaque caract√©ristique.

* Une **cha√Æne de r√©f√©rence** est une **colonne qui identifie un exemple** (comme le nom d'une personne). Cela peut √™tre dupliqu√© (car 2 personnes peuvent avoir le m√™me nom) mais la plupart seront uniques. Ces donn√©es sont **inutiles et doivent √™tre supprim√©es**.
* Une **colonne cl√©** est utilis√©e pour **lier des donn√©es entre les tables**. Dans ce cas, les √©l√©ments sont uniques. Ces donn√©es sont **inutiles et doivent √™tre supprim√©es**.

Pour **encoder des colonnes multi-cat√©gories en nombres** (afin que l'algorithme ML les comprenne), on utilise **l'encodage factice** (et **non l'encodage one-hot** car cela **n'√©vite pas la multicollin√©arit√© parfaite**).

Vous pouvez obtenir une **colonne multi-cat√©gorie encod√©e en one-hot** avec `pd.get_dummies(dataset.column1)`. Cela transformera toutes les classes en caract√©ristiques binaires, cr√©ant ainsi **une nouvelle colonne par classe possible** et attribuera 1 **valeur Vraie √† une colonne**, les autres √©tant fausses.

Vous pouvez obtenir une **colonne multi-cat√©gorie encod√©e en factice** avec `pd.get_dummies(dataset.column1, drop_first=True)`. Cela transformera toutes les classes en caract√©ristiques binaires, cr√©ant ainsi **une nouvelle colonne par classe possible moins une** car **les deux derni√®res colonnes refl√©teront "1" ou "0" dans la derni√®re colonne binaire cr√©√©e**. Cela √©vitera la multicollin√©arit√© parfaite, r√©duisant les relations entre les colonnes.

# Colin√©aire/Multicollin√©arit√©

La colin√©arit√© se produit lorsque **2 caract√©ristiques sont li√©es entre elles**. La multicollin√©arit√© se produit lorsqu'il y en a plus de 2.

En ML, **vous voulez que vos caract√©ristiques soient li√©es aux r√©sultats possibles mais pas entre elles**. C'est pourquoi l'**encodage factice m√©lange les deux derni√®res colonnes** de celle-ci et **est meilleur que l'encodage one-hot** qui ne le fait pas, cr√©ant une relation claire entre toutes les nouvelles caract√©ristiques de la colonne multi-cat√©gorie.

Le VIF est le **Facteur d'Inflation de la Variance** qui **mesure la multicollin√©arit√© des caract√©ristiques**. Une valeur **sup√©rieure √† 5 signifie qu'une des deux ou plusieurs caract√©ristiques colin√©aires doit √™tre supprim√©e**.
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# D√©s√©quilibre cat√©goriel

Cela se produit lorsqu'il n'y a **pas le m√™me nombre de chaque cat√©gorie** dans les donn√©es d'entra√Ænement.
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
Dans un d√©s√©quilibre, il y a toujours une **classe ou des classes majoritaires** et une **classe ou des classes minoritaires**.

Il existe 2 principales fa√ßons de r√©soudre ce probl√®me :

* **Sous-√©chantillonnage** : Supprimer des donn√©es s√©lectionn√©es de mani√®re al√©atoire de la classe majoritaire afin qu'elle ait le m√™me nombre d'√©chantillons que la classe minoritaire.
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Sur√©chantillonnage**: G√©n√©rer plus de donn√©es pour la classe minoritaire jusqu'√† ce qu'elle ait autant d'√©chantillons que la classe majoritaire.
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
Vous pouvez utiliser l'argument **`sampling_strategy`** pour indiquer le **pourcentage** que vous souhaitez **sous-√©chantillonner ou sur-√©chantillonner** (**par d√©faut, c'est 1 (100%)** ce qui signifie √©galiser le nombre de classes minoritaires avec les classes majoritaires)

{% hint style="info" %}
Le sous-√©chantillonnage ou le sur-√©chantillonnage ne sont pas parfaits. Si vous obtenez des statistiques (avec `.describe()`) des donn√©es sur-√©chantillonn√©es/sous-√©chantillonn√©es et que vous les comparez √† l'original, vous verrez **qu'elles ont chang√©**. Par cons√©quent, le sur-√©chantillonnage et le sous-√©chantillonnage modifient les donn√©es d'entra√Ænement.
{% endhint %}

## Sur-√©chantillonnage SMOTE

**SMOTE** est g√©n√©ralement une **m√©thode plus fiable pour sur-√©chantillonner les donn√©es**.
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# Cat√©gories Rarement Occurrentes

Imaginez un ensemble de donn√©es o√π l'une des classes cibles **se produit tr√®s rarement**.

C'est comme le d√©s√©quilibre des cat√©gories de la section pr√©c√©dente, mais la cat√©gorie rarement pr√©sente se produit encore moins que la "classe minoritaire" dans ce cas. Les m√©thodes de **sur√©chantillonnage** et de **sous-√©chantillonnage** brutes pourraient √©galement √™tre utilis√©es ici, mais en g√©n√©ral ces techniques **ne donneront pas vraiment de bons r√©sultats**.

## Poids

Dans certains algorithmes, il est possible de **modifier les poids des donn√©es cibl√©es** afin que certaines d'entre elles aient par d√©faut plus d'importance lors de la g√©n√©ration du mod√®le.
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
Vous pouvez **m√©langer les poids avec des techniques de sur/ sous-√©chantillonnage** pour essayer d'am√©liorer les r√©sultats.

## ACP - Analyse en Composantes Principales

Est une m√©thode qui aide √† r√©duire la dimensionnalit√© des donn√©es. Elle va **combiner diff√©rentes caract√©ristiques** pour **r√©duire la quantit√©** d'entre elles en g√©n√©rant **des caract√©ristiques plus utiles** (_moins de calculs sont n√©cessaires_).

Les caract√©ristiques r√©sultantes ne sont pas compr√©hensibles par les humains, donc cela **anonymise √©galement les donn√©es**.

# Cat√©gories d'√©tiquettes incoh√©rentes

Les donn√©es peuvent comporter des erreurs dues √† des transformations infructueuses ou simplement √† des erreurs humaines lors de la saisie des donn√©es.

Par cons√©quent, vous pourriez trouver la **m√™me √©tiquette avec des fautes d'orthographe**, des **diff√©rences de casse**, des **abr√©viations** telles que : _BLEU, Bleu, b, bule_. Vous devez corriger ces erreurs d'√©tiquetage dans les donn√©es avant d'entra√Æner le mod√®le.

Vous pouvez r√©soudre ces probl√®mes en mettant tout en minuscules et en faisant correspondre les √©tiquettes mal orthographi√©es aux bonnes.

Il est tr√®s important de v√©rifier que **toutes les donn√©es que vous avez sont correctement √©tiquet√©es**, car par exemple, une erreur de faute d'orthographe dans les donn√©es, lors de l'encodage des classes factices, g√©n√©rera une nouvelle colonne dans les caract√©ristiques finales avec **de mauvaises cons√©quences pour le mod√®le final**. Cet exemple peut √™tre d√©tect√© tr√®s facilement en encodant √† chaud une colonne et en v√©rifiant les noms des colonnes cr√©√©es.

# Donn√©es manquantes

Certaines donn√©es de l'√©tude peuvent √™tre manquantes.

Il se peut que certaines donn√©es al√©atoires compl√®tes manquent pour une erreur quelconque. Ce type de donn√©es est **Manquant Compl√®tement au Hasard** (**MCAR**).

Il se pourrait que certaines donn√©es al√©atoires manquent mais qu'il y ait quelque chose qui rend plus probable que certains d√©tails sp√©cifiques manquent, par exemple, les hommes diront plus fr√©quemment leur √¢ge que les femmes. Cela s'appelle **Manquant au Hasard** (**MAR**).

Enfin, il pourrait y avoir des donn√©es **Manquantes Non au Hasard** (**MNAR**). La valeur des donn√©es est directement li√©e √† la probabilit√© d'avoir les donn√©es. Par exemple, si vous voulez mesurer quelque chose de g√™nant, plus quelqu'un est g√™n√©, moins il est probable qu'il le partage.

Les **deux premi√®res cat√©gories** de donn√©es manquantes peuvent √™tre **ignor√©es**. Mais la **troisi√®me** n√©cessite de consid√©rer **seulement des portions des donn√©es** qui ne sont pas impact√©es ou d'essayer de **mod√©liser d'une certaine mani√®re les donn√©es manquantes**.

Une fa√ßon de d√©tecter les donn√©es manquantes est d'utiliser la fonction `.info()` car elle indiquera le **nombre de lignes mais aussi le nombre de valeurs par cat√©gorie**. Si une cat√©gorie a moins de valeurs que le nombre de lignes, alors il manque des donn√©es :
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
Il est g√©n√©ralement recommand√© que si une fonctionnalit√© est **manquante dans plus de 20 %** de l'ensemble de donn√©es, la **colonne devrait √™tre supprim√©e :**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
Notez que **toutes les valeurs manquantes ne sont pas absentes dans l'ensemble de donn√©es**. Il est possible que les valeurs manquantes aient √©t√© donn√©es avec la valeur "Inconnu", "n/a", "", -1, 0... Vous devez v√©rifier l'ensemble de donn√©es (en utilisant `ensemble.de.donn√©es.nom.colonne.valeur.comptes(dropna=False)` pour v√©rifier les valeurs possibles).
{% endhint %}

Si des donn√©es manquent dans l'ensemble de donn√©es (et que ce n'est pas trop), vous devez trouver la **cat√©gorie des donn√©es manquantes**. Pour cela, vous devez essentiellement savoir si les **donn√©es manquantes sont al√©atoires ou non**, et pour cela, vous devez d√©terminer si les **donn√©es manquantes √©taient corr√©l√©es avec d'autres donn√©es** de l'ensemble de donn√©es.

Pour d√©terminer si une valeur manquante est corr√©l√©e avec une autre colonne, vous pouvez cr√©er une nouvelle colonne qui met des 1 et des 0 si les donn√©es sont manquantes ou non, puis calculer la corr√©lation entre elles :
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
Si vous d√©cidez d'ignorer les donn√©es manquantes, vous devez quand m√™me d√©cider quoi en faire : vous pouvez **supprimer les lignes** avec des donn√©es manquantes (les donn√©es d'entra√Ænement du mod√®le seront plus petites), vous pouvez **supprimer compl√®tement la caract√©ristique**, ou vous pouvez **la mod√©liser**.

Vous devriez **v√©rifier la corr√©lation entre la caract√©ristique manquante et la colonne cible** pour voir √† quel point cette caract√©ristique est importante pour la cible, si elle est vraiment **faible**, vous pouvez **la supprimer ou la remplir**.

Pour remplir les donn√©es continues manquantes, vous pourriez utiliser : la **moyenne**, la **m√©diane** ou utiliser un **algorithme d'imputation**. L'algorithme d'imputation peut essayer d'utiliser d'autres caract√©ristiques pour trouver une valeur pour la caract√©ristique manquante :
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
Pour remplir les donn√©es cat√©gorielles, tout d'abord, vous devez r√©fl√©chir s'il y a une raison pour laquelle les valeurs sont manquantes. Si c'est par **choix des utilisateurs** (ils ne voulaient pas fournir les donn√©es), vous pouvez peut-√™tre **cr√©er une nouvelle cat√©gorie** l'indiquant. S'il s'agit d'une erreur humaine, vous pouvez **supprimer les lignes** ou la **caract√©ristique** (v√©rifiez les √©tapes mentionn√©es pr√©c√©demment) ou **la remplir avec le mode, la cat√©gorie la plus utilis√©e** (non recommand√©).

# Combinaison de caract√©ristiques

Si vous trouvez **deux caract√©ristiques** qui sont **corr√©l√©es** entre elles, vous devriez g√©n√©ralement **en abandonner une** (celle qui est moins corr√©l√©e avec la cible), mais vous pourriez √©galement essayer de **les combiner et cr√©er une nouvelle caract√©ristique**.
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>Apprenez le piratage AWS de z√©ro √† h√©ros avec</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (Expert en √©quipe rouge AWS de HackTricks)</strong></a><strong>!</strong></summary>

Autres fa√ßons de soutenir HackTricks:

* Si vous souhaitez voir votre **entreprise annonc√©e dans HackTricks** ou **t√©l√©charger HackTricks en PDF**, consultez les [**PLANS D'ABONNEMENT**](https://github.com/sponsors/carlospolop)!
* Obtenez le [**swag officiel PEASS & HackTricks**](https://peass.creator-spring.com)
* D√©couvrez [**La famille PEASS**](https://opensea.io/collection/the-peass-family), notre collection exclusive de [**NFTs**](https://opensea.io/collection/the-peass-family)
* **Rejoignez le** üí¨ [**groupe Discord**](https://discord.gg/hRep4RUj7f) ou le [**groupe Telegram**](https://t.me/peass) ou **suivez-nous** sur **Twitter** üê¶ [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Partagez vos astuces de piratage en soumettant des PR aux** [**HackTricks**](https://github.com/carlospolop/hacktricks) et [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) d√©p√¥ts GitHub.

</details>
