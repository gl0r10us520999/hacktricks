<details>

<summary><strong>Aprenda hacking AWS do zero ao her√≥i com</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Outras maneiras de apoiar o HackTricks:

* Se voc√™ quiser ver sua **empresa anunciada no HackTricks** ou **baixar o HackTricks em PDF** Confira os [**PLANOS DE ASSINATURA**](https://github.com/sponsors/carlospolop)!
* Adquira o [**swag oficial PEASS & HackTricks**](https://peass.creator-spring.com)
* Descubra [**A Fam√≠lia PEASS**](https://opensea.io/collection/the-peass-family), nossa cole√ß√£o exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)
* **Junte-se ao** üí¨ [**grupo Discord**](https://discord.gg/hRep4RUj7f) ou ao [**grupo telegram**](https://t.me/peass) ou **siga-nos** no **Twitter** üê¶ [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Compartilhe seus truques de hacking enviando PRs para os** [**HackTricks**](https://github.com/carlospolop/hacktricks) e [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) reposit√≥rios do github.

</details>


# Tipos b√°sicos de dados poss√≠veis

Os dados podem ser **cont√≠nuos** (valores **infinitos**) ou **categ√≥ricos** (nominais) onde a quantidade de valores poss√≠veis √© **limitada**.

## Tipos Categ√≥ricos

### Bin√°rio

Apenas **2 valores poss√≠veis**: 1 ou 0. No caso de um conjunto de dados em que os valores est√£o em formato de string (por exemplo, "Verdadeiro" e "Falso"), voc√™ atribui n√∫meros a esses valores com:
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **Ordinal**

Os **valores seguem uma ordem**, como em: 1¬∫ lugar, 2¬∫ lugar... Se as categorias s√£o strings (como: "iniciante", "amador", "profissional", "especialista") voc√™ pode mape√°-las para n√∫meros como vimos no caso bin√°rio.
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* Para **colunas alfab√©ticas** voc√™ pode orden√°-las mais facilmente:
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **C√≠clico**

Parece **um valor ordinal** porque h√° uma ordem, mas isso n√£o significa que um seja maior que o outro. Al√©m disso, a **dist√¢ncia entre eles depende da dire√ß√£o** que voc√™ est√° contando. Exemplo: Os dias da semana, domingo n√£o √© "maior" que segunda-feira.

* Existem **diferentes maneiras** de codificar caracter√≠sticas c√≠clicas, algumas podem funcionar apenas com alguns algoritmos. **Em geral, a codifica√ß√£o dummy pode ser usada**.
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **Datas**

Datas s√£o **vari√°veis** **cont√≠nuas**. Podem ser vistas como **c√≠clicas** (porque se repetem) **ou** como vari√°veis **ordinais** (porque um tempo √© maior que outro anterior).

* Geralmente as datas s√£o usadas como **√≠ndice**
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### Multi-categoria/nominal

**Mais de 2 categorias** sem ordem relacionada. Use `dataset.describe(include='all')` para obter informa√ß√µes sobre as categorias de cada recurso.

* Uma **string de refer√™ncia** √© uma **coluna que identifica um exemplo** (como o nome de uma pessoa). Isso pode ser duplicado (porque 2 pessoas podem ter o mesmo nome), mas a maioria ser√° √∫nica. Esses dados s√£o **in√∫teis e devem ser removidos**.
* Uma **coluna chave** √© usada para **vincular dados entre tabelas**. Neste caso, os elementos s√£o √∫nicos. Esses dados s√£o **in√∫teis e devem ser removidos**.

Para **codificar colunas de m√∫ltiplas categorias em n√∫meros** (para que o algoritmo de ML as entenda), √© usada a **codifica√ß√£o dummy** (e **n√£o a codifica√ß√£o one-hot** porque **n√£o evita a multicolinearidade perfeita**).

Voc√™ pode obter uma **coluna de m√∫ltiplas categorias codificada one-hot** com `pd.get_dummies(dataset.column1)`. Isso transformar√° todas as classes em recursos bin√°rios, criando **uma nova coluna para cada classe poss√≠vel** e atribuir√° 1 **valor verdadeiro a uma coluna**, e o restante ser√° falso.

Voc√™ pode obter uma **coluna de m√∫ltiplas categorias codificada dummy** com `pd.get_dummies(dataset.column1, drop_first=True)`. Isso transformar√° todas as classes em recursos bin√°rios, criando **uma nova coluna para cada classe poss√≠vel menos uma**, pois as **√∫ltimas 2 colunas refletir√£o "1" ou "0" na √∫ltima coluna bin√°ria criada**. Isso evitar√° a multicolinearidade perfeita, reduzindo as rela√ß√µes entre as colunas.

# Colinear/Multicolinearidade

Colinear aparece quando **2 recursos est√£o relacionados entre si**. Multicolinearidade aparece quando h√° mais de 2.

No ML, **voc√™ deseja que seus recursos estejam relacionados com os resultados poss√≠veis, mas n√£o deseja que estejam relacionados entre si**. Por isso, a **codifica√ß√£o dummy mistura as duas √∫ltimas colunas** disso e **√© melhor do que a codifica√ß√£o one-hot**, que n√£o faz isso, criando uma rela√ß√£o clara entre todos os novos recursos da coluna de m√∫ltiplas categorias.

VIF √© o **Fator de Infla√ß√£o da Vari√¢ncia** que **mede a multicolinearidade dos recursos**. Um valor **acima de 5 significa que um dos dois ou mais recursos colineares deve ser removido**.
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# Desequil√≠brio Categ√≥rico

Isso ocorre quando **n√£o h√° a mesma quantidade de cada categoria** nos dados de treinamento.
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
Em um desequil√≠brio, sempre h√° uma **classe ou classes majorit√°rias** e uma **classe ou classes minorit√°rias**.

Existem 2 maneiras principais de resolver esse problema:

* **Undersampling**: Remover dados selecionados aleatoriamente da classe majorit√°ria para que ela tenha o mesmo n√∫mero de amostras que a classe minorit√°ria.
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Oversampling**: Gerar mais dados para a classe minorit√°ria at√© que ela tenha tantas amostras quanto a classe majorit√°ria.
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
Pode usar o argumento **`sampling_strategy`** para indicar a **porcentagem** que deseja **subamostrar ou sobreamostrar** (**por padr√£o √© 1 (100%)** o que significa igualar o n√∫mero de classes minorit√°rias com as classes majorit√°rias)

{% hint style="info" %}
A subamostragem ou sobreamostragem n√£o s√£o perfeitas, se voc√™ obter estat√≠sticas (com `.describe()`) dos dados sobreamostrados/subamostrados e compar√°-los com os originais, ver√° **que eles mudaram**. Portanto, a sobreamostragem e subamostragem est√£o modificando os dados de treinamento.
{% endhint %}

## Sobreamostragem SMOTE

**SMOTE** √© geralmente uma **maneira mais confi√°vel de sobreamostrar os dados**.
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# Categorias Raramente Ocorrentes

Imagine um conjunto de dados onde uma das classes alvo ocorre muito poucas vezes.

Isso √© semelhante ao desequil√≠brio de categorias da se√ß√£o anterior, mas a categoria raramente ocorrente ocorre ainda menos do que a "classe minorit√°ria" nesse caso. Os m√©todos de **sobreamostragem** e **subamostragem** **brutos** tamb√©m podem ser usados aqui, mas geralmente essas t√©cnicas **n√£o fornecer√£o resultados realmente bons**.

## Pesos

Em alguns algoritmos, √© poss√≠vel **modificar os pesos dos dados alvo** para que alguns deles tenham por padr√£o mais import√¢ncia ao gerar o modelo.
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
Pode **misturar os pesos com t√©cnicas de oversampling/undersampling** para tentar melhorar os resultados.

## PCA - An√°lise de Componentes Principais

√â um m√©todo que ajuda a reduzir a dimensionalidade dos dados. Vai **combinar diferentes caracter√≠sticas** para **reduzir a quantidade** delas gerando **caracter√≠sticas mais √∫teis** (_√© necess√°ria menos computa√ß√£o_).

As caracter√≠sticas resultantes n√£o s√£o compreens√≠veis por humanos, ent√£o tamb√©m **anonimiza os dados**.

# Categorias de R√≥tulos Incongruentes

Os dados podem ter erros devido a transforma√ß√µes mal sucedidas ou apenas por erro humano ao escrever os dados.

Portanto, voc√™ pode encontrar o **mesmo r√≥tulo com erros de digita√ß√£o**, **diferentes mai√∫sculas**, **abrevia√ß√µes** como: _AZUL, Azul, a, azul_. Voc√™ precisa corrigir esses erros de r√≥tulo nos dados antes de treinar o modelo.

Voc√™ pode resolver esses problemas colocando tudo em min√∫sculas e mapeando r√≥tulos com erros de digita√ß√£o para os corretos.

√â muito importante verificar se **todos os dados que voc√™ possui est√£o corretamente rotulados**, porque, por exemplo, um erro de digita√ß√£o nos dados, ao codificar as classes, ir√° gerar uma nova coluna nas caracter√≠sticas finais com **consequ√™ncias ruins para o modelo final**. Esse exemplo pode ser detectado facilmente codificando um atributo e verificando os nomes das colunas criadas.

# Dados Ausentes

Alguns dados do estudo podem estar faltando.

Pode acontecer que alguns dados completos estejam faltando por algum erro. Esse tipo de dado √© **Completamente Ausente de Forma Aleat√≥ria** (**MCAR**).

Pode ser que alguns dados aleat√≥rios estejam faltando, mas algo est√° tornando alguns detalhes espec√≠ficos mais prov√°veis de estar faltando, por exemplo, os homens frequentemente informam sua idade, mas n√£o as mulheres. Isso √© chamado de **Ausente de Forma Aleat√≥ria** (**MAR**).

Por fim, pode haver dados **N√£o Ausentes de Forma Aleat√≥ria** (**MNAR**). O valor dos dados est√° diretamente relacionado com a probabilidade de ter os dados. Por exemplo, se voc√™ quiser medir algo embara√ßoso, quanto mais embara√ßoso algu√©m for, menos prov√°vel √© que ele v√° compartilhar.

As **duas primeiras categorias** de dados ausentes podem ser **ignoradas**. Mas a **terceira** requer considerar **apenas por√ß√µes dos dados** que n√£o s√£o impactadas ou tentar **modelar os dados ausentes de alguma forma**.

Uma maneira de descobrir sobre dados ausentes √© usar a fun√ß√£o `.info()` pois indicar√° o **n√∫mero de linhas, mas tamb√©m o n√∫mero de valores por categoria**. Se alguma categoria tiver menos valores do que o n√∫mero de linhas, ent√£o h√° dados faltando:
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
Geralmente √© recomendado que se uma caracter√≠stica est√° **ausente em mais de 20%** do conjunto de dados, a **coluna deve ser removida:**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
Note que **nem todos os valores ausentes est√£o faltando no conjunto de dados**. √â poss√≠vel que os valores ausentes tenham sido atribu√≠dos como "Desconhecido", "n/a", "", -1, 0... Voc√™ precisa verificar o conjunto de dados (usando `conjunto_de_dados.nome_da_coluna.valor_contagens(dropna=False)` para verificar os poss√≠veis valores).
{% endhint %}

Se alguns dados estiverem faltando no conjunto de dados (e n√£o forem muitos), voc√™ precisa encontrar a **categoria dos dados ausentes**. Para isso, basicamente voc√™ precisa saber se os **dados ausentes est√£o em aleat√≥rio ou n√£o**, e para isso voc√™ precisa descobrir se os **dados ausentes estavam correlacionados com outros dados** do conjunto de dados.

Para descobrir se um valor ausente est√° correlacionado com outra coluna, voc√™ pode criar uma nova coluna que coloca 1s e 0s se os dados est√£o faltando ou n√£o e ent√£o calcular a correla√ß√£o entre eles:
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
Se decidir ignorar os dados em falta, ainda precisa fazer o que com eles: Voc√™ pode **remover as linhas** com dados em falta (os dados de treino para o modelo ser√£o menores), pode **remover completamente a feature**, ou pode **model√°-la**.

Voc√™ deve **verificar a correla√ß√£o entre a feature em falta com a coluna alvo** para ver o qu√£o importante essa feature √© para o alvo, se for realmente **pequena**, voc√™ pode **descart√°-la ou preench√™-la**.

Para preencher dados cont√≠nuos em falta, voc√™ pode usar: a **m√©dia**, a **mediana** ou usar um **algoritmo de imputa√ß√£o**. O algoritmo de imputa√ß√£o pode tentar usar outras features para encontrar um valor para a feature em falta:
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
Para preencher dados categ√≥ricos, primeiro voc√™ precisa pensar se h√° alguma raz√£o pela qual os valores est√£o faltando. Se for por **escolha dos usu√°rios** (eles n√£o quiseram fornecer os dados), talvez voc√™ possa **criar uma nova categoria** indicando isso. Se for por erro humano, voc√™ pode **remover as linhas** ou a **caracter√≠stica** (verifique os passos mencionados anteriormente) ou **preench√™-la com a moda, a categoria mais utilizada** (n√£o recomendado).

# Combinando Caracter√≠sticas

Se voc√™ encontrar **duas caracter√≠sticas** que est√£o **correlacionadas** entre si, geralmente voc√™ deve **descartar** uma delas (a menos correlacionada com o alvo), mas tamb√©m pode tentar **combin√°-las e criar uma nova caracter√≠stica**.
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>Aprenda hacking AWS do zero ao her√≥i com</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Outras maneiras de apoiar o HackTricks:

* Se voc√™ deseja ver sua **empresa anunciada no HackTricks** ou **baixar o HackTricks em PDF** Confira os [**PLANOS DE ASSINATURA**](https://github.com/sponsors/carlospolop)!
* Adquira o [**swag oficial PEASS & HackTricks**](https://peass.creator-spring.com)
* Descubra [**A Fam√≠lia PEASS**](https://opensea.io/collection/the-peass-family), nossa cole√ß√£o exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)
* **Junte-se ao** üí¨ [**grupo Discord**](https://discord.gg/hRep4RUj7f) ou ao [**grupo telegram**](https://t.me/peass) ou **siga-nos** no **Twitter** üê¶ [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Compartilhe seus truques de hacking enviando PRs para os** [**HackTricks**](https://github.com/carlospolop/hacktricks) e [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) reposit√≥rios do github.

</details>
