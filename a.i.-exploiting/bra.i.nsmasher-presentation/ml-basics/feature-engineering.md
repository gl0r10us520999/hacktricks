<details>

<summary><strong>Aprende hacking en AWS desde cero hasta convertirte en un h칠roe con</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (Experto en Red Team de AWS de HackTricks)</strong></a><strong>!</strong></summary>

Otras formas de apoyar a HackTricks:

* Si deseas ver tu **empresa anunciada en HackTricks** o **descargar HackTricks en PDF** Consulta los [**PLANES DE SUSCRIPCI칍N**](https://github.com/sponsors/carlospolop)!
* Obt칠n la [**merchandising oficial de PEASS & HackTricks**](https://peass.creator-spring.com)
* Descubre [**La Familia PEASS**](https://opensea.io/collection/the-peass-family), nuestra colecci칩n exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)
* **칔nete al** 游눫 [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **s칤guenos** en **Twitter** 游냕 [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Comparte tus trucos de hacking enviando PRs a los** [**HackTricks**](https://github.com/carlospolop/hacktricks) y [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) repositorios de github.

</details>


# Tipos b치sicos de datos posibles

Los datos pueden ser **continuos** (valores **infinitos**) o **categ칩ricos** (nominales) donde la cantidad de valores posibles es **limitada**.

## Tipos categ칩ricos

### Binario

Solo **2 valores posibles**: 1 o 0. En caso de que en un conjunto de datos los valores est칠n en formato de cadena (por ejemplo, "Verdadero" y "Falso") asignas n칰meros a esos valores con:
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **Ordinal**

Los **valores siguen un orden**, como en: 1er lugar, 2do lugar... Si las categor칤as son cadenas de texto (como: "principiante", "amateur", "profesional", "experto") puedes mapearlos a n칰meros como vimos en el caso binario.
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* Para las **columnas alfab칠ticas** puedes ordenarlas m치s f치cilmente:
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **C칤clico**

Parece **un valor ordinal** porque hay un orden, pero no significa que uno sea m치s grande que el otro. Adem치s, la **distancia entre ellos depende de la direcci칩n** en la que se cuentan. Ejemplo: Los d칤as de la semana, el domingo no es "m치s grande" que el lunes.

* Hay **diferentes formas** de codificar caracter칤sticas c칤clicas, algunas pueden funcionar solo con algunos algoritmos. **En general, se puede utilizar la codificaci칩n dummy**.
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **Fechas**

Las fechas son **variables** **continuas**. Pueden ser vistas como **c칤clicas** (porque se repiten) **o** como variables **ordinales** (porque un tiempo es mayor que otro anterior).

* Usualmente las fechas se utilizan como **칤ndice**.
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### Multi-categor칤a/nominal

**M치s de 2 categor칤as** sin un orden relacionado. Utiliza `dataset.describe(include='all')` para obtener informaci칩n sobre las categor칤as de cada caracter칤stica.

* Una **cadena de referencia** es una **columna que identifica un ejemplo** (como el nombre de una persona). Esto puede estar duplicado (porque 2 personas pueden tener el mismo nombre) pero la mayor칤a ser치 칰nico. Estos datos son **in칰tiles y deben ser eliminados**.
* Una **columna clave** se utiliza para **vincular datos entre tablas**. En este caso, los elementos son 칰nicos. Estos datos son **in칰tiles y deben ser eliminados**.

Para **codificar columnas de m칰ltiples categor칤as en n칰meros** (para que el algoritmo de ML las entienda), se utiliza **codificaci칩n dummy** (y **no codificaci칩n one-hot** porque **no evita la multicolinealidad perfecta**).

Puedes obtener una **columna de m칰ltiples categor칤as codificada en one-hot** con `pd.get_dummies(dataset.column1)`. Esto transformar치 todas las clases en caracter칤sticas binarias, creando **una nueva columna por cada clase posible** y asignar치 un valor de 1 **verdadero a una columna**, y el resto ser치 falso.

Puedes obtener una **columna de m칰ltiples categor칤as codificada en dummies** con `pd.get_dummies(dataset.column1, drop_first=True)`. Esto transformar치 todas las clases en caracter칤sticas binarias, creando **una nueva columna por cada clase posible menos una** ya que **las 칰ltimas 2 columnas se reflejar치n como "1" o "0" en la 칰ltima columna binaria creada**. Esto evitar치 la multicolinealidad perfecta, reduciendo las relaciones entre columnas.

# Colineal/Multicolinealidad

La colinealidad aparece cuando **2 caracter칤sticas est치n relacionadas entre s칤**. La multicolinealidad aparece cuando hay m치s de 2.

En ML **quieres que tus caracter칤sticas est칠n relacionadas con los posibles resultados pero no quieres que est칠n relacionadas entre s칤**. Por eso la **codificaci칩n dummy mezcla las 칰ltimas dos columnas** de eso y **es mejor que la codificaci칩n one-hot** que no hace eso creando una clara relaci칩n entre todas las nuevas caracter칤sticas de la columna de m칰ltiples categor칤as.

VIF es el **Factor de Inflaci칩n de la Varianza** que **mide la multicolinealidad de las caracter칤sticas**. Un valor **superior a 5 significa que una de las dos o m치s caracter칤sticas colineales debe ser eliminada**.
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# Desequilibrio Categ칩rico

Esto ocurre cuando **no hay la misma cantidad de cada categor칤a** en los datos de entrenamiento.
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
En un desequilibrio siempre hay una **clase o clases mayoritarias** y una **clase o clases minoritarias**.

Hay 2 formas principales de solucionar este problema:

* **Submuestreo**: Eliminar datos seleccionados al azar de la clase mayoritaria para que tenga el mismo n칰mero de muestras que la clase minoritaria.
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Sobremuestreo**: Generar m치s datos para la clase minoritaria hasta que tenga tantas muestras como la clase mayoritaria.
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
Puedes usar el argumento **`sampling_strategy`** para indicar el **porcentaje** que deseas **submuestrear o sobremuestrear** (**por defecto es 1 (100%)** lo que significa igualar el n칰mero de clases minoritarias con las clases mayoritarias)

{% hint style="info" %}
El submuestreo o sobremuestreo no son perfectos, si obtienes estad칤sticas (con `.describe()`) de los datos sobremuestreados o submuestreados y los comparas con los originales, ver치s **que han cambiado**. Por lo tanto, el sobremuestreo y submuestreo modifican los datos de entrenamiento.
{% endhint %}

## Sobremuestreo SMOTE

**SMOTE** es generalmente una **forma m치s confiable de sobremuestrear los datos**.
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# Categor칤as que ocurren raramente

Imagina un conjunto de datos donde una de las clases objetivo **ocurre muy pocas veces**.

Esto es similar al desequilibrio de categor칤as de la secci칩n anterior, pero la categor칤a que ocurre raramente ocurre incluso menos que la "clase minoritaria" en ese caso. Los m칠todos de **sobremuestreo** y **submuestreo** **brutos** tambi칠n podr칤an ser utilizados aqu칤, pero generalmente esas t칠cnicas **no dar치n resultados realmente buenos**.

## Pesos

En algunos algoritmos es posible **modificar los pesos de los datos objetivo** para que algunos de ellos tengan por defecto m치s importancia al generar el modelo.
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
Puedes **mezclar los pesos con t칠cnicas de sobre/muestreo** para intentar mejorar los resultados.

## PCA - An치lisis de Componentes Principales

Es un m칠todo que ayuda a reducir la dimensionalidad de los datos. Va a **combinar diferentes caracter칤sticas** para **reducir la cantidad** de ellas generando **caracter칤sticas m치s 칰tiles** (_se necesita menos c치lculo_).

Las caracter칤sticas resultantes no son comprensibles por los humanos, por lo que tambi칠n **anonimizan los datos**.

# Categor칤as de Etiquetas Incongruentes

Los datos pueden tener errores por transformaciones fallidas o simplemente por error humano al escribir los datos.

Por lo tanto, es posible encontrar la **misma etiqueta con errores de ortograf칤a**, diferentes **may칰sculas**, **abreviaturas** como: _AZUL, Azul, a, azul_. Debes corregir estos errores de etiqueta dentro de los datos antes de entrenar el modelo.

Puedes solucionar estos problemas convirtiendo todo a min칰sculas y mapeando las etiquetas mal escritas a las correctas.

Es muy importante verificar que **todos los datos que tienes est칠n etiquetados correctamente**, porque por ejemplo, un error de ortograf칤a en los datos, al codificar en dummies las clases, generar치 una nueva columna en las caracter칤sticas finales con **consecuencias negativas para el modelo final**. Este ejemplo se puede detectar muy f치cilmente codificando en one-hot una columna y verificando los nombres de las columnas creadas.

# Datos Faltantes

Algunos datos del estudio pueden faltar.

Puede suceder que falten algunos datos aleatorios por alg칰n error. Este tipo de datos faltantes es **Faltante Completamente al Azar** (**MCAR**).

Podr칤a ser que falten algunos datos aleatorios pero haya algo que haga que algunos detalles espec칤ficos sean m치s probables de faltar, por ejemplo, es m치s probable que un hombre diga su edad pero no una mujer. Esto se llama **Faltante al Azar** (**MAR**).

Finalmente, podr칤a haber datos **Faltantes No al Azar** (**MNAR**). El valor de los datos est치 directamente relacionado con la probabilidad de tener los datos. Por ejemplo, si quieres medir algo vergonzoso, cuanto m치s vergonzosa sea una persona, menos probable es que lo comparta.

Las **dos primeras categor칤as** de datos faltantes pueden ser **ignoradas**. Pero la **tercera** requiere considerar **solo porciones de los datos** que no se ven afectadas o intentar **modelar de alguna manera los datos faltantes**.

Una forma de detectar datos faltantes es usar la funci칩n `.info()` ya que indicar치 el **n칰mero de filas pero tambi칠n el n칰mero de valores por categor칤a**. Si alguna categor칤a tiene menos valores que el n칰mero de filas, entonces faltan algunos datos:
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
Generalmente se recomienda que si una caracter칤stica falta en m치s del **20%** del conjunto de datos, la **columna debe ser eliminada:**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
Ten en cuenta que **no todos los valores faltantes est치n ausentes en el conjunto de datos**. Es posible que los valores faltantes hayan sido asignados con el valor "Desconocido", "n/a", "", -1, 0... Necesitas verificar el conjunto de datos (usando `conjunto_de_datos.nombre_de_columna.valor_contados(dropna=False)` para verificar los posibles valores).
{% endhint %}

Si falta algo de datos en el conjunto de datos (y no es demasiado), necesitas encontrar la **categor칤a de los datos faltantes**. Para eso, b치sicamente necesitas saber si los **datos faltantes est치n al azar o no**, y para eso necesitas averiguar si los **datos faltantes estaban correlacionados con otros datos** del conjunto de datos.

Para determinar si un valor faltante est치 correlacionado con otra columna, puedes crear una nueva columna que ponga 1s y 0s si los datos faltan o no, y luego calcular la correlaci칩n entre ellos:
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
Si decides ignorar los datos faltantes, a칰n necesitas decidir qu칠 hacer con ellos: puedes **eliminar las filas** con datos faltantes (los datos de entrenamiento del modelo ser치n m치s peque침os), puedes **eliminar por completo la caracter칤stica**, o puedes **modelarla**.

Debes **verificar la correlaci칩n entre la caracter칤stica faltante y la columna objetivo** para ver qu칠 tan importante es esa caracter칤stica para el objetivo, si es realmente **peque침a**, puedes **eliminarla o completarla**.

Para completar datos faltantes **continuos** podr칤as usar: la **media**, la **mediana** o utilizar un **algoritmo de imputaci칩n**. El algoritmo de imputaci칩n puede intentar usar otras caracter칤sticas para encontrar un valor para la caracter칤stica faltante:
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
Para completar los datos categ칩ricos, primero debes pensar si hay alguna raz칩n por la cual faltan los valores. Si es por **elecci칩n de los usuarios** (no quisieron proporcionar los datos), tal vez puedas **crear una nueva categor칤a** indic치ndolo. Si es debido a un error humano, puedes **eliminar las filas** o la **caracter칤stica** (verificar los pasos mencionados anteriormente) o **rellenarla con la moda, la categor칤a m치s utilizada** (no recomendado).

# Combinaci칩n de Caracter칤sticas

Si encuentras **dos caracter칤sticas** que est치n **correlacionadas** entre s칤, generalmente deber칤as **eliminar** una de ellas (la menos correlacionada con el objetivo), pero tambi칠n podr칤as intentar **combinarlas y crear una nueva caracter칤stica**.
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>Aprende hacking en AWS desde cero hasta experto con</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Otras formas de apoyar a HackTricks:

* Si quieres ver tu **empresa anunciada en HackTricks** o **descargar HackTricks en PDF** Consulta los [**PLANES DE SUSCRIPCI칍N**](https://github.com/sponsors/carlospolop)!
* Obt칠n el [**oficial PEASS & HackTricks swag**](https://peass.creator-spring.com)
* Descubre [**The PEASS Family**](https://opensea.io/collection/the-peass-family), nuestra colecci칩n de [**NFTs**](https://opensea.io/collection/the-peass-family) exclusivos
* **칔nete al** 游눫 [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **s칤guenos** en **Twitter** 游냕 [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Comparte tus trucos de hacking enviando PRs a los** [**HackTricks**](https://github.com/carlospolop/hacktricks) y [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) repositorios de github.

</details>
