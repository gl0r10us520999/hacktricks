# рдореИрд▓реЛрдХ рдФрд░ рд╕рд┐рд╕рдореИрд▓реЛрдХ

{% hint style="success" %}
AWS рд╣реИрдХрд┐рдВрдЧ рд╕реАрдЦреЗрдВ рдФрд░ рдЕрднреНрдпрд╛рд╕ рдХрд░реЗрдВ:<img src="/.gitbook/assets/arte.png" alt="" data-size="line">[**HackTricks рдкреНрд░рд╢рд┐рдХреНрд╖рдг AWS рд░реЗрдб рдЯреАрдо рд╡рд┐рд╢реЗрд╖рдЬреНрдЮ (ARTE)**](https://training.hacktricks.xyz/courses/arte)<img src="/.gitbook/assets/arte.png" alt="" data-size="line">\
GCP рд╣реИрдХрд┐рдВрдЧ рд╕реАрдЦреЗрдВ рдФрд░ рдЕрднреНрдпрд╛рд╕ рдХрд░реЗрдВ: <img src="/.gitbook/assets/grte.png" alt="" data-size="line">[**HackTricks рдкреНрд░рд╢рд┐рдХреНрд╖рдг GCP рд░реЗрдб рдЯреАрдо рд╡рд┐рд╢реЗрд╖рдЬреНрдЮ (GRTE)**<img src="/.gitbook/assets/grte.png" alt="" data-size="line">](https://training.hacktricks.xyz/courses/grte)

<details>

<summary>рд╣реИрдХрдЯреНрд░рд┐рдХреНрд╕ рдХрд╛ рд╕рдорд░реНрдерди рдХрд░реЗрдВ</summary>

* [**рд╕рджрд╕реНрдпрддрд╛ рдпреЛрдЬрдирд╛рдПрдВ**](https://github.com/sponsors/carlospolop) рдХреА рдЬрд╛рдБрдЪ рдХрд░реЗрдВ!
* **рд╢рд╛рдорд┐рд▓ рд╣реЛрдВ** ЁЯТм [**рдбрд┐рд╕реНрдХреЙрд░реНрдб рд╕рдореВрд╣**](https://discord.gg/hRep4RUj7f) рдпрд╛ [**рдЯреЗрд▓реАрдЧреНрд░рд╛рдо рд╕рдореВрд╣**](https://t.me/peass) рдпрд╛ рд╣рдореЗрдВ **рдЯреНрд╡рд┐рдЯрд░** ЁЯРж [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)** рдкрд░ рдлреЙрд▓реЛ** рдХрд░реЗрдВред
* **рд╣реИрдХрд┐рдВрдЧ рдЯреНрд░рд┐рдХреНрд╕ рд╕рд╛рдЭрд╛ рдХрд░реЗрдВ, рд╣реИрдХрдЯреНрд░рд┐рдХреНрд╕**](https://github.com/carlospolop/hacktricks) рдФрд░ [**рд╣реИрдХрдЯреНрд░рд┐рдХреНрд╕ рдХреНрд▓рд╛рдЙрдб**](https://github.com/carlospolop/hacktricks-cloud) github рд░реЗрдкреЛ рдореЗрдВ PR рдЬрдорд╛ рдХрд░рдХреЗред

</details>
{% endhint %}

## рдЖрд╡рдВрдЯрди рдХреНрд░рдо рд╕рд╛рд░рд╛рдВрд╢ <a href="#libc_malloc" id="libc_malloc"></a>

(рдЗрд╕ рд╕рд╛рд░рд╛рдВрд╢ рдореЗрдВ рдХреЛрдИ рдЬрд╛рдВрдЪ рдирд╣реАрдВ рдХреА рдЧрдИ рд╣реИ рдФрд░ рдХреБрдЫ рдорд╛рдорд▓реЗ рд╕рдВрдХреНрд╖реЗрдк рдХреЗ рд▓рд┐рдП рдЫреЛрдбрд╝ рджрд┐рдП рдЧрдП рд╣реИрдВ)

1. `__libc_malloc` рдЯреАрдХреИрд╢ рд╕реЗ рдПрдХ рдЪрдВрдХ рдкреНрд░рд╛рдкреНрдд рдХрд░рдиреЗ рдХрд╛ рдкреНрд░рдпрд╛рд╕ рдХрд░рддрд╛ рд╣реИ, рдЕрдЧрд░ рдирд╣реАрдВ рддреЛ рд╡рд╣ `_int_malloc` рдХреЛ рдХреЙрд▓ рдХрд░рддрд╛ рд╣реИ
2. `_int_malloc` :&#x20;
1. рдПрд░реАрдирд╛ рдЙрддреНрдкрдиреНрди рдХрд░рдиреЗ рдХрд╛ рдкреНрд░рдпрд╛рд╕ рдХрд░рддрд╛ рд╣реИ рдЕрдЧрд░ рдХреЛрдИ рдирд╣реАрдВ рд╣реИ
2. рдпрджрд┐ рдХреЛрдИ рд╕рд╣реА рдЖрдХрд╛рд░ рдХрд╛ рдлрд╛рд╕реНрдЯ рдмрд┐рди рдЪрдВрдХ рд╣реИ, рддреЛ рдЙрд╕рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ
1. рдЕрдиреНрдп рдлрд╛рд╕реНрдЯ рдЪрдВрдХ рдХреЗ рд╕рд╛рде рдЯреАрдХреИрд╢ рднрд░реЗрдВ
3. рдпрджрд┐ рдХреЛрдИ рдЫреЛрдЯрд╛ рдмрд┐рди рдЪрдВрдХ рд╕рд╣реА рдЖрдХрд╛рд░ рдХрд╛ рд╣реИ, рддреЛ рдЙрд╕рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ
1. рдЙрд╕ рдЖрдХрд╛рд░ рдХреЗ рдЕрдиреНрдп рдЪрдВрдХ рдХреЗ рд╕рд╛рде рдЯреАрдХреИрд╢ рднрд░реЗрдВ
4. рдпрджрд┐ рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рдЫреЛрдЯреЗ рдмрд┐рдиреЛрдВ рдХреЗ рд▓рд┐рдП рдирд╣реАрдВ рд╣реИ, рддреЛ рдлрд╛рд╕реНрдЯ рдмрд┐рди рдХреЛ рдЕрдирд╕реЙрд░реНрдЯреЗрдб рдмрд┐рди рдореЗрдВ рд╕рдореЗрдЯреЗрдВ
5. рдЕрдирд╕реЙрд░реНрдЯреЗрдб рдмрд┐рди рдХреА рдЬрд╛рдВрдЪ рдХрд░реЗрдВ, рдкрд╣рд▓рд╛ рдЪрдВрдХ рдЙрдкрдпреБрдХреНрдд рд╕реНрдерд╛рди рдХреЗ рд╕рд╛рде рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ
1. рдпрджрд┐ рдкрд╛рдпрд╛ рдЧрдпрд╛ рдЪрдВрдХ рдмрдбрд╝рд╛ рд╣реИ, рддреЛ рдЙрд╕реЗ рд╡рд┐рднрд╛рдЬрд┐рдд рдХрд░рдХреЗ рдПрдХ рд╣рд┐рд╕реНрд╕рд╛ рд╡рд╛рдкрд╕ рдХрд░рдиреЗ рдФрд░ рдмрд╛рдХреА рдХреЛ рдЕрдирд╕реЙрд░реНрдЯреЗрдб рдмрд┐рди рдореЗрдВ рдЬреЛрдбрд╝рдиреЗ рдХреЗ рд▓рд┐рдП
2. рдпрджрд┐ рдПрдХ рдЪрдВрдХ рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рдХреЗ рд╕рдорд╛рди рд╣реИ, рддреЛ рдЗрд╕реЗ рд╡рд╛рдкрд╕ рджреЗрдиреЗ рдХреА рдмрдЬрд╛рдп рдЯреАрдХреИрд╢ рднрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ (рдЬрдм рддрдХ рдЯреАрдХреИрд╢ рднрд░рд╛ рдирд╣реАрдВ рд╣реЛрддрд╛, рддреЛ рдЕрдЧрд▓рд╛ рд╡рд╛рдкрд╕ рдХрд░реЗрдВ)
3. рдЫреЛрдЯреЗ рдпрд╛ рдмрдбрд╝реЗ рдЖрдХрд╛рд░ рдХреЗ рдкреНрд░рддреНрдпреЗрдХ рдЪрдВрдХ рдХреЗ рд▓рд┐рдП рдЬрд╛рдВрдЪ рдХрд░рдиреЗ рдкрд░, рдЙрд╕реЗ рдЙрд╕рдХреЗ рд╕рдВрдмрдВрдзрд┐рдд рдЫреЛрдЯреЗ рдпрд╛ рдмрдбрд╝реЗ рдмрд┐рди рдореЗрдВ рдбрд╛рд▓реЗрдВ
6. рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рдХреЗ рд╕реВрдЪрдХрд╛рдВрдХ рдореЗрдВ рдмрдбрд╝реЗ рдмрд┐рди рдХреА рдЬрд╛рдВрдЪ рдХрд░реЗрдВ
1. рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рд╕реЗ рдмрдбрд╝рд╛ рдкрд╣рд▓рд╛ рдЪрдВрдХ рд╕реЗ рджреЗрдЦрдирд╛ рд╢реБрд░реВ рдХрд░реЗрдВ, рдпрджрд┐ рдХреЛрдИ рдорд┐рд▓рддрд╛ рд╣реИ рддреЛ рдЙрд╕реЗ рд╡рд╛рдкрд╕ рдХрд░реЗрдВ рдФрд░ рдмрд╛рдХреА рдХреЛ рдЫреЛрдЯреЗ рдмрд┐рди рдореЗрдВ рдЬреЛрдбрд╝реЗрдВ
7. рдЕрдВрдд рддрдХ рдмрдбрд╝реЗ рдмрд┐рди рдХреА рдЬрд╛рдВрдЪ рдХрд░реЗрдВ
1. рдЕрдЧрд▓реЗ рдмрдбрд╝реЗ рд╕реВрдЪрдХрд╛рдВрдХ рд╕реЗ рдХрд┐рд╕реА рднреА рдЪрдВрдХ рдХреА рдЬрд╛рдВрдЪ рдХрд░реЗрдВ, рдкрд╣рд▓рд╛ рдкрд╛рдпрд╛ рдЧрдпрд╛ рдЪрдВрдХ рдЙрдкрдпреЛрдЧ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рд╡рд┐рднрд╛рдЬрд┐рдд рдХрд░реЗрдВ рдФрд░ рдмрд╛рдХреА рдХреЛ рдЕрдирд╕реЙрд░реНрдЯреЗрдб рдмрд┐рди рдореЗрдВ рдЬреЛрдбрд╝реЗрдВ
8. рдкрд┐рдЫрд▓реЗ рдмрд┐рдиреЛрдВ рдореЗрдВ рдХреБрдЫ рднреА рдирд╣реАрдВ рдорд┐рд▓рд╛ рд╣реЛ, рддреЛ рдКрдкрд░ рд╕реЗ рдПрдХ рдЪрдВрдХ рдкреНрд░рд╛рдкреНрдд рдХрд░реЗрдВ
9. рдпрджрд┐ рдКрдкрд░ рд╕реЗ рдкреНрд░рд╛рдкреНрдд рдЪрдВрдХ рдкрд░реНрдпрд╛рдкреНрдд рдмрдбрд╝рд╛ рдирд╣реАрдВ рдерд╛, рддреЛ `sysmalloc` рдХреЗ рд╕рд╛рде рдЗрд╕реЗ рдмрдбрд╝рд╛ рдХрд░реЗрдВ

## \_\_libc\_malloc <a href="#libc_malloc" id="libc_malloc"></a>

`malloc` рдлрд╝рдВрдХреНрд╢рди рд╡рд╛рд╕реНрддрд╡ рдореЗрдВ `__libc_malloc` рдХреЛ рдХреЙрд▓ рдХрд░рддрд╛ рд╣реИред рдпрд╣ рдлрд╝рдВрдХреНрд╢рди рдЯреАрдХреИрд╢ рдХреА рдЬрд╛рдВрдЪ рдХрд░реЗрдЧрд╛ рдХрд┐ рдХреНрдпрд╛ рдЪрд╛рд╣рд┐рдП рдЧрдП рдЖрдХрд╛рд░ рдХрд╛ рдХреЛрдИ рдЙрдкрд▓рдмреНрдз рдЪрдВрдХ рд╣реИред рдпрджрд┐ рд╣рд╛рдВ, рддреЛ рдпрд╣ рдЙрд╕рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдЧрд╛ рдФрд░ рдпрджрд┐ рдирд╣реАрдВ, рддреЛ рдпрд╣ рджреЗрдЦреЗрдЧрд╛ рдХрд┐ рдХреНрдпрд╛ рдпрд╣ рдПрдХрд▓ рдзрд╛рдЧрд╛ рд╣реИ рдФрд░ рдЙрд╕ рдорд╛рдорд▓реЗ рдореЗрдВ рдпрд╣ рдореБрдЦреНрдп рдПрд░реАрдирд╛ рдореЗрдВ `_int_malloc` рдХреЛ рдХреЙрд▓ рдХрд░реЗрдЧрд╛, рдФрд░ рдпрджрд┐ рдирд╣реАрдВ, рддреЛ рдпрд╣ рдзрд╛рдЧреЗ рдХреЗ рдПрд░реАрдирд╛ рдореЗрдВ `_int_malloc` рдХреЛ рдХреЙрд▓ рдХрд░реЗрдЧрд╛ред

<details>

<summary>__libc_malloc рдХреЛрдб</summary>
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c

#if IS_IN (libc)
void *
__libc_malloc (size_t bytes)
{
mstate ar_ptr;
void *victim;

_Static_assert (PTRDIFF_MAX <= SIZE_MAX / 2,
"PTRDIFF_MAX is not more than half of SIZE_MAX");

if (!__malloc_initialized)
ptmalloc_init ();
#if USE_TCACHE
/* int_free also calls request2size, be careful to not pad twice.  */
size_t tbytes = checked_request2size (bytes);
if (tbytes == 0)
{
__set_errno (ENOMEM);
return NULL;
}
size_t tc_idx = csize2tidx (tbytes);

MAYBE_INIT_TCACHE ();

DIAG_PUSH_NEEDS_COMMENT;
if (tc_idx < mp_.tcache_bins
&& tcache != NULL
&& tcache->counts[tc_idx] > 0)
{
victim = tcache_get (tc_idx);
return tag_new_usable (victim);
}
DIAG_POP_NEEDS_COMMENT;
#endif

if (SINGLE_THREAD_P)
{
victim = tag_new_usable (_int_malloc (&main_arena, bytes));
assert (!victim || chunk_is_mmapped (mem2chunk (victim)) ||
&main_arena == arena_for_chunk (mem2chunk (victim)));
return victim;
}

arena_get (ar_ptr, bytes);

victim = _int_malloc (ar_ptr, bytes);
/* Retry with another arena only if we were able to find a usable arena
before.  */
if (!victim && ar_ptr != NULL)
{
LIBC_PROBE (memory_malloc_retry, 1, bytes);
ar_ptr = arena_get_retry (ar_ptr, bytes);
victim = _int_malloc (ar_ptr, bytes);
}

if (ar_ptr != NULL)
__libc_lock_unlock (ar_ptr->mutex);

victim = tag_new_usable (victim);

assert (!victim || chunk_is_mmapped (mem2chunk (victim)) ||
ar_ptr == arena_for_chunk (mem2chunk (victim)));
return victim;
}
```
</details>

рдиреЛрдЯ рдХрд░реЗрдВ рдХрд┐ рдпрд╣ рд╣рдореЗрд╢рд╛ рд╡рд╛рдкрд╕ рдЖрдиреЗ рд╡рд╛рд▓реЗ рдкреЙрдЗрдВрдЯрд░ рдХреЛ `tag_new_usable` рдХреЗ рд╕рд╛рде рдЯреИрдЧ рдХрд░реЗрдЧрд╛, рдХреЛрдб рд╕реЗ:
```c
void *tag_new_usable (void *ptr)

Allocate a new random color and use it to color the user region of
a chunk; this may include data from the subsequent chunk's header
if tagging is sufficiently fine grained.  Returns PTR suitably
recolored for accessing the memory there.
```
## \_int\_malloc <a href="#int_malloc" id="int_malloc"></a>

рдпрд╣ рдлрд╝рдВрдХреНрд╢рди рдЕрдиреНрдп рдмрд┐рди рдФрд░ рд╢реАрд░реНрд╖ рдЪрдВрдХ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ рдореЗрдореЛрд░реА рдХрд╛ рдЖрд╡рдВрдЯрди рдХрд░рддрд╛ рд╣реИред

* рдкреНрд░рд╛рд░рдВрдн

рдпрд╣ рдХреБрдЫ рд╡реЗрд░рд┐рдПрдмрд▓реНрд╕ рдХреА рдкрд░рд┐рднрд╛рд╖рд╛ рд╢реБрд░реВ рдХрд░рддрд╛ рд╣реИ рдФрд░ рдЕрдиреБрд░реЛрдзрд┐рдд рдореЗрдореЛрд░реА рд╕реНрдерд╛рди рдХреА рд╡рд╛рд╕реНрддрд╡рд┐рдХ рд╕рд╛рдЗрдЬрд╝ рдкреНрд░рд╛рдкреНрдд рдХрд░рддрд╛ рд╣реИ:
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L3847
static void *
_int_malloc (mstate av, size_t bytes)
{
INTERNAL_SIZE_T nb;               /* normalized request size */
unsigned int idx;                 /* associated bin index */
mbinptr bin;                      /* associated bin */

mchunkptr victim;                 /* inspected/selected chunk */
INTERNAL_SIZE_T size;             /* its size */
int victim_index;                 /* its bin index */

mchunkptr remainder;              /* remainder from a split */
unsigned long remainder_size;     /* its size */

unsigned int block;               /* bit map traverser */
unsigned int bit;                 /* bit map traverser */
unsigned int map;                 /* current word of binmap */

mchunkptr fwd;                    /* misc temp for linking */
mchunkptr bck;                    /* misc temp for linking */

#if USE_TCACHE
size_t tcache_unsorted_count;	    /* count of unsorted chunks processed */
#endif

/*
Convert request size to internal form by adding SIZE_SZ bytes
overhead plus possibly more to obtain necessary alignment and/or
to obtain a size of at least MINSIZE, the smallest allocatable
size. Also, checked_request2size returns false for request sizes
that are so large that they wrap around zero when padded and
aligned.
*/

nb = checked_request2size (bytes);
if (nb == 0)
{
__set_errno (ENOMEM);
return NULL;
}
```
</details>

### рдХреНрд╖реЗрддреНрд░

рдпрджрд┐ рдЙрдкрдпреЛрдЧреА рдХреНрд╖реЗрддреНрд░ рдирд╣реАрдВ рд╣реИ, рддреЛ `sysmalloc` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ `mmap` рд╕реЗ рдЯреБрдХрдбрд╝рд╛ рдкреНрд░рд╛рдкреНрдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдЗрд╕рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИ:
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L3885C3-L3893C6
/* There are no usable arenas.  Fall back to sysmalloc to get a chunk from
mmap.  */
if (__glibc_unlikely (av == NULL))
{
void *p = sysmalloc (nb, av);
if (p != NULL)
alloc_perturb (p, bytes);
return p;
}
```
</details>

### рддреНрд╡рд░рд┐рдд рдмрд┐рди

рдпрджрд┐ рдЖрд╡рд╢реНрдпрдХ рдЖрдХрд╛рд░ рддреНрд╡рд░рд┐рдд рдмрд┐рди рдЖрдХрд╛рд░реЛрдВ рдХреЗ рдЕрдВрджрд░ рд╣реИ, рддреЛ рддреНрд╡рд░рд┐рдд рдмрд┐рди рд╕реЗ рдПрдХ рдЪрдВрдХ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдиреЗ рдХрд╛ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВред рдореВрд▓ рд░реВрдк рд╕реЗ, рдЖрдХрд╛рд░ рдХреЗ рдЖрдзрд╛рд░ рдкрд░, рдпрд╣ рд╡реИрдз рдЪрдВрдХ рдХрд╣рд╛рдБ рд╕реНрдерд┐рдд рд╣реЛрдиреЗ рдЪрд╛рд╣рд┐рдП, рдЙрд╕ рддреНрд╡рд░рд┐рдд рдмрд┐рди рд╕реВрдЪрдХрд╛рдВрдХ рдХреЛ рдЦреЛрдЬреЗрдЧрд╛, рдФрд░ рдпрджрд┐ рдХреЛрдИ рд╣реЛ, рддреЛ рд╡рд╣ рдЙрдирдореЗрдВ рд╕реЗ рдПрдХ рд▓реМрдЯрд╛рдПрдЧрд╛ред\
рдЗрд╕рдХреЗ рдЕрддрд┐рд░рд┐рдХреНрдд, рдпрджрд┐ рдЯреАрдХреИрд╢ рдЪрд╛рд▓реВ рд╣реИ, рддреЛ рдпрд╣ **рдЙрд╕ рдЖрдХрд╛рд░ рдХреЗ рдЯреАрдХреИрд╢ рдмрд┐рди рдХреЛ рддреНрд╡рд░рд┐рдд рдмрд┐рди рдХреЗ рд╕рд╛рде рднрд░реЗрдЧрд╛**ред

рдЬрдм рдпреЗ рдХреНрд░рд┐рдпрд╛рдПрдБ рдХреА рдЬрд╛рддреА рд╣реИрдВ, рддреЛ рдпрд╣рд╛рдБ рдХреБрдЫ рд╕реБрд░рдХреНрд╖рд╛ рдЬрд╛рдВрдЪреЗрдВ рдХреА рдЬрд╛рддреА рд╣реИрдВ:

* рдпрджрд┐ рдЪрдВрдХ рдЧрд▓рдд рд░реВрдк рд╕реЗ рд╕реНрдерд┐рдд рд╣реИ: `malloc(): unaligned fastbin chunk detected 2`
* рдпрджрд┐ рдЖрдЧреЗ рдХрд╛ рдЪрдВрдХ рдЧрд▓рдд рд░реВрдк рд╕реЗ рд╕реНрдерд┐рдд рд╣реИ: `malloc(): unaligned fastbin chunk detected`
* рдпрджрд┐ рд▓реМрдЯрд╛рдпрд╛ рдЧрдпрд╛ рдЪрдВрдХ рдЙрд╕ рдЖрдХрд╛рд░ рдХрд╛ рд╣реИ рдЬреЛ рддреНрд╡рд░рд┐рдд рдмрд┐рди рдореЗрдВ рдЙрд╕рдХреЗ рд╕реВрдЪрдХрд╛рдВрдХ рдХреЗ рдХрд╛рд░рдг рд╕рд╣реА рдирд╣реАрдВ рд╣реИ: `malloc(): memory corruption (fast)`
* рдпрджрд┐ рдХреЛрдИ рднреА рдЪрдВрдХ рдЬреЛ рдЯреАрдХреИрд╢ рднрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИ, рдЧрд▓рдд рд░реВрдк рд╕реЗ рд╕реНрдерд┐рдд рд╣реИ: `malloc(): unaligned fastbin chunk detected 3`

<details>

<summary>_int_malloc fast bin</summary>
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L3895C3-L3967C6
/*
If the size qualifies as a fastbin, first check corresponding bin.
This code is safe to execute even if av is not yet initialized, so we
can try it without checking, which saves some time on this fast path.
*/

#define REMOVE_FB(fb, victim, pp)			\
do							\
{							\
victim = pp;					\
if (victim == NULL)				\
break;						\
pp = REVEAL_PTR (victim->fd);                                     \
if (__glibc_unlikely (pp != NULL && misaligned_chunk (pp)))       \
malloc_printerr ("malloc(): unaligned fastbin chunk detected"); \
}							\
while ((pp = catomic_compare_and_exchange_val_acq (fb, pp, victim)) \
!= victim);					\

if ((unsigned long) (nb) <= (unsigned long) (get_max_fast ()))
{
idx = fastbin_index (nb);
mfastbinptr *fb = &fastbin (av, idx);
mchunkptr pp;
victim = *fb;

if (victim != NULL)
{
if (__glibc_unlikely (misaligned_chunk (victim)))
malloc_printerr ("malloc(): unaligned fastbin chunk detected 2");

if (SINGLE_THREAD_P)
*fb = REVEAL_PTR (victim->fd);
else
REMOVE_FB (fb, pp, victim);
if (__glibc_likely (victim != NULL))
{
size_t victim_idx = fastbin_index (chunksize (victim));
if (__builtin_expect (victim_idx != idx, 0))
malloc_printerr ("malloc(): memory corruption (fast)");
check_remalloced_chunk (av, victim, nb);
#if USE_TCACHE
/* While we're here, if we see other chunks of the same size,
stash them in the tcache.  */
size_t tc_idx = csize2tidx (nb);
if (tcache != NULL && tc_idx < mp_.tcache_bins)
{
mchunkptr tc_victim;

/* While bin not empty and tcache not full, copy chunks.  */
while (tcache->counts[tc_idx] < mp_.tcache_count
&& (tc_victim = *fb) != NULL)
{
if (__glibc_unlikely (misaligned_chunk (tc_victim)))
malloc_printerr ("malloc(): unaligned fastbin chunk detected 3");
if (SINGLE_THREAD_P)
*fb = REVEAL_PTR (tc_victim->fd);
else
{
REMOVE_FB (fb, pp, tc_victim);
if (__glibc_unlikely (tc_victim == NULL))
break;
}
tcache_put (tc_victim, tc_idx);
}
}
#endif
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}
}
}
```
</details>

### рдЫреЛрдЯрд╛ рдмрд┐рди

рдПрдХ рдЯрд┐рдкреНрдкрдгреА рдореЗрдВ рдЗрдВрдбрд┐рдХреЗрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИ рдХрд┐ рдЫреЛрдЯреЗ рдмрд┐рдиреНрд╕ рдкреНрд░рддрд┐ рдЗрдВрдбреЗрдХреНрд╕ рдПрдХ рд╕рд╛рдЗрдЬрд╝ рдзрд╛рд░рд┐рдд рдХрд░рддреЗ рд╣реИрдВ, рдЗрд╕рд▓рд┐рдП рдпрджрд┐ рдПрдХ рд╡реИрдз рдЪрдВрдХ рдЙрдкрд▓рдмреНрдз рд╣реИ рддреЛ рдЙрд╕реЗ рдЬрд╛рдВрдЪрдирд╛ рдмрд╣реБрдд рддреЗрдЬреА рд╕реЗ рд╣реЛрддрд╛ рд╣реИ, рдЗрд╕рд▓рд┐рдП рдлрд╛рд╕реНрдЯ рдмрд┐рдиреНрд╕ рдХреЗ рдмрд╛рдж, рдЫреЛрдЯреЗ рдмрд┐рдиреНрд╕ рдХреА рдЬрд╛рдВрдЪ рдХреА рдЬрд╛рддреА рд╣реИред

рдкрд╣рд▓реА рдЬрд╛рдВрдЪ рдпрд╣ рд╣реИ рдХрд┐ рдХреНрдпрд╛ рдЕрдиреБрд░реЛрдзрд┐рдд рд╕рд╛рдЗрдЬрд╝ рдПрдХ рдЫреЛрдЯреЗ рдмрд┐рди рдореЗрдВ рд╣реЛ рд╕рдХрддрд╛ рд╣реИред рдЙрд╕ рдорд╛рдорд▓реЗ рдореЗрдВ, рдЫреЛрдЯреЗ рдмрд┐рди рдХреЗ рдЕрдВрджрд░ рд╕рдВрдмрдВрдзрд┐рдд **рдЗрдВрдбреЗрдХреНрд╕** рдкреНрд░рд╛рдкреНрдд рдХрд░реЗрдВ рдФрд░ рджреЗрдЦреЗрдВ рдХрд┐ рдХреНрдпрд╛ **рдХреЛрдИ рдЙрдкрд▓рдмреНрдз рдЪрдВрдХ** рд╣реИред

рдлрд┐рд░, рдПрдХ рд╕реБрд░рдХреНрд╖рд╛ рдЬрд╛рдВрдЪ рдХреА рдЬрд╛рддреА рд╣реИ рдЬрд┐рд╕рдореЗрдВ рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рд╢рд╛рдорд┐рд▓ рд╣реИ:

* &#x20;рдпрджрд┐ `victim->bk->fd = victim`. рджреЗрдЦрдиреЗ рдХреЗ рд▓рд┐рдП рдХрд┐ рдХреНрдпрд╛ рджреЛрдиреЛрдВ рдЪрдВрдХ рд╕рд╣реА рдврдВрдЧ рд╕реЗ рд▓рд┐рдВрдХ рд╣реИрдВред

рдЙрд╕ рдорд╛рдорд▓реЗ рдореЗрдВ, рдЪрдВрдХ **рдХреЛ `inuse` рдмрд┐рдЯ рдорд┐рд▓рддрд╛ рд╣реИ,** рдбрдмрд▓ рд▓рд┐рдВрдХреНрдб рд╕реВрдЪреА рдареАрдХ рд╣реЛ рдЬрд╛рддреА рд╣реИ рддрд╛рдХрд┐ рдпрд╣ рдЪрдВрдХ рдЙрд╕рд╕реЗ рдЧрд╛рдпрдм рд╣реЛ рдЬрд╛рддрд╛ рд╣реИ (рдХреНрдпреЛрдВрдХрд┐ рдЗрд╕рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рдПрдЧрд╛), рдФрд░ рдЧреИрд░ рдореБрдЦреНрдп рдПрд░рд┐рдирд╛ рдмрд┐рдЯ рд╕реЗрдЯ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ рдпрджрд┐ рдЖрд╡рд╢реНрдпрдХ рд╣реЛред

рдЕрдВрддрддрдГ, **рдЕрдиреБрд░реЛрдзрд┐рдд рд╕рд╛рдЗрдЬрд╝ рдХреЗ tcache рдЗрдВрдбреЗрдХреНрд╕ рдХреЛ** рдЫреЛрдЯреЗ рдмрд┐рди рдХреЗ рдЕрдВрджрд░ рдЕрдиреНрдп рдЪрдВрдХреНрд╕ рд╕реЗ рднрд░реЗрдВ (рдпрджрд┐ рдХреЛрдИ рд╣реЛ)ред

<details>

<summary>_int_malloc small bin</summary>
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L3895C3-L3967C6

/*
If a small request, check regular bin.  Since these "smallbins"
hold one size each, no searching within bins is necessary.
(For a large request, we need to wait until unsorted chunks are
processed to find best fit. But for small ones, fits are exact
anyway, so we can check now, which is faster.)
*/

if (in_smallbin_range (nb))
{
idx = smallbin_index (nb);
bin = bin_at (av, idx);

if ((victim = last (bin)) != bin)
{
bck = victim->bk;
if (__glibc_unlikely (bck->fd != victim))
malloc_printerr ("malloc(): smallbin double linked list corrupted");
set_inuse_bit_at_offset (victim, nb);
bin->bk = bck;
bck->fd = bin;

if (av != &main_arena)
set_non_main_arena (victim);
check_malloced_chunk (av, victim, nb);
#if USE_TCACHE
/* While we're here, if we see other chunks of the same size,
stash them in the tcache.  */
size_t tc_idx = csize2tidx (nb);
if (tcache != NULL && tc_idx < mp_.tcache_bins)
{
mchunkptr tc_victim;

/* While bin not empty and tcache not full, copy chunks over.  */
while (tcache->counts[tc_idx] < mp_.tcache_count
&& (tc_victim = last (bin)) != bin)
{
if (tc_victim != 0)
{
bck = tc_victim->bk;
set_inuse_bit_at_offset (tc_victim, nb);
if (av != &main_arena)
set_non_main_arena (tc_victim);
bin->bk = bck;
bck->fd = bin;

tcache_put (tc_victim, tc_idx);
}
}
}
#endif
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}
}
```
</details>

### malloc\_consolidate

рдЕрдЧрд░ рдпрд╣ рдПрдХ рдЫреЛрдЯрд╛ рдЪрдВрдХ рдирд╣реАрдВ рдерд╛, рддреЛ рдпрд╣ рдПрдХ рдмрдбрд╝рд╛ рдЪрдВрдХ рд╣реИ, рдФрд░ рдЗрд╕ рдорд╛рдорд▓реЗ рдореЗрдВ **`malloc_consolidate`** рдХреЛ рдореЗрдореЛрд░реА рдлреНрд░реЗрдЧрдореЗрдВрдЯреЗрд╢рди рд╕реЗ рдмрдЪрдиреЗ рдХреЗ рд▓рд┐рдП рдХреЙрд▓ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред

<details>

<summary>malloc_consolidate рдХреЙрд▓</summary>
```c
/*
If this is a large request, consolidate fastbins before continuing.
While it might look excessive to kill all fastbins before
even seeing if there is space available, this avoids
fragmentation problems normally associated with fastbins.
Also, in practice, programs tend to have runs of either small or
large requests, but less often mixtures, so consolidation is not
invoked all that often in most programs. And the programs that
it is called frequently in otherwise tend to fragment.
*/

else
{
idx = largebin_index (nb);
if (atomic_load_relaxed (&av->have_fastchunks))
malloc_consolidate (av);
}

```
</details>

malloc рд╕рдореЗрдХрди рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдмреБрдирд┐рдпрд╛рджреА рд░реВрдк рд╕реЗ рддреЗрдЬ рдмрд┐рди рд╕реЗ рдЯреБрдХрдбрд╝реЗ рд╣рдЯрд╛рддреА рд╣реИ рдФрд░ рдЙрдиреНрд╣реЗрдВ рдЕрд╕рдВрдЧрдард┐рдд рдмрд┐рди рдореЗрдВ рд░рдЦрддреА рд╣реИред рдЕрдЧрд▓реЗ malloc рдХреЗ рдмрд╛рдж рдпреЗ рдЯреБрдХрдбрд╝реЗ рдЕрдкрдиреЗ рд╕рдВрдмрдВрдзрд┐рдд рдЫреЛрдЯреЗ/рддреЗрдЬ рдмрд┐рдиреЛрдВ рдореЗрдВ рд╕рдВрдЧрдард┐рдд рд╣реЛ рдЬрд╛рдПрдВрдЧреЗред

рдзреНрдпрд╛рди рджреЗрдВ рдХрд┐ рдЬрдм рдЗрди рдЯреБрдХрдбрд╝реЛрдВ рдХреЛ рд╣рдЯрд╛рддреЗ рд╕рдордп, рдЕрдЧрд░ рдЙрдиреНрд╣реЗрдВ рдкрд┐рдЫрд▓реЗ рдпрд╛ рдЕрдЧрд▓реЗ рдЯреБрдХрдбрд╝реЛрдВ рдХреЗ рд╕рд╛рде рдкрд╛рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ рдЬреЛ рдЙрдкрдпреЛрдЧ рдореЗрдВ рдирд╣реАрдВ рд╣реИрдВ, рддреЛ рд╡реЗ рдЕрдВрдд рдореЗрдВ рдЯреБрдХрдбрд╝реЗ рдХреЛ рдЕрд╕рдВрдЧрдард┐рдд рдмрд┐рди рдореЗрдВ рд░рдЦрдиреЗ рд╕реЗ рдкрд╣рд▓реЗ **рдЕрд▓рд╛рдЗрдХ рдФрд░ рдорд░реНрдЬ** рдХрд┐рдП рдЬрд╛рдПрдВрдЧреЗред

рдкреНрд░рддреНрдпреЗрдХ рддреЗрдЬ рдмрд┐рди рдЯреБрдХрдбрд╝реЗ рдХреЗ рд▓рд┐рдП рдХреБрдЫ рд╕реБрд░рдХреНрд╖рд╛ рдЬрд╛рдВрдЪреЗрдВ рдХреА рдЬрд╛рддреА рд╣реИрдВ:

* рдЕрдЧрд░ рдЯреБрдХрдбрд╝рд╛ рдЕрдЕрд▓рд╛рдЗрдиреНрдб рд╣реИ рддреЛ рдЯреНрд░рд┐рдЧрд░ рдХрд░реЗрдВ: `malloc_consolidate(): unaligned fastbin chunk detected`
* рдЕрдЧрд░ рдЯреБрдХрдбрд╝рд╛ рдЙрд╕ рдЖрдХрд╛рд░ рдХрд╛ рдирд╣реАрдВ рд╣реИ рдЬреЛ рдЗрдВрдбреЗрдХреНрд╕ рдХреЗ рдХрд╛рд░рдг рд╣реЛрдирд╛ рдЪрд╛рд╣рд┐рдП: `malloc_consolidate(): invalid chunk size`
* рдЕрдЧрд░ рдкрд┐рдЫрд▓рд╛ рдЯреБрдХрдбрд╝рд╛ рдЙрдкрдпреЛрдЧ рдореЗрдВ рдирд╣реАрдВ рд╣реИ рдФрд░ рдкрд┐рдЫрд▓реЗ рдЯреБрдХрдбрд╝реЗ рдХрд╛ рдЖрдХрд╛рд░ `prev_chunk` рджреНрд╡рд╛рд░рд╛ рдирд┐рд░реНрджрд┐рд╖реНрдЯ рдЖрдХрд╛рд░ рд╕реЗ рднрд┐рдиреНрди рд╣реИ: `corrupted size vs. prev_size in fastbins`

<details>

<summary>malloc_consolidate рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛</summary>
```c
// https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L4810C1-L4905C2

static void malloc_consolidate(mstate av)
{
mfastbinptr*    fb;                 /* current fastbin being consolidated */
mfastbinptr*    maxfb;              /* last fastbin (for loop control) */
mchunkptr       p;                  /* current chunk being consolidated */
mchunkptr       nextp;              /* next chunk to consolidate */
mchunkptr       unsorted_bin;       /* bin header */
mchunkptr       first_unsorted;     /* chunk to link to */

/* These have same use as in free() */
mchunkptr       nextchunk;
INTERNAL_SIZE_T size;
INTERNAL_SIZE_T nextsize;
INTERNAL_SIZE_T prevsize;
int             nextinuse;

atomic_store_relaxed (&av->have_fastchunks, false);

unsorted_bin = unsorted_chunks(av);

/*
Remove each chunk from fast bin and consolidate it, placing it
then in unsorted bin. Among other reasons for doing this,
placing in unsorted bin avoids needing to calculate actual bins
until malloc is sure that chunks aren't immediately going to be
reused anyway.
*/

maxfb = &fastbin (av, NFASTBINS - 1);
fb = &fastbin (av, 0);
do {
p = atomic_exchange_acquire (fb, NULL);
if (p != 0) {
do {
{
if (__glibc_unlikely (misaligned_chunk (p)))
malloc_printerr ("malloc_consolidate(): "
"unaligned fastbin chunk detected");

unsigned int idx = fastbin_index (chunksize (p));
if ((&fastbin (av, idx)) != fb)
malloc_printerr ("malloc_consolidate(): invalid chunk size");
}

check_inuse_chunk(av, p);
nextp = REVEAL_PTR (p->fd);

/* Slightly streamlined version of consolidation code in free() */
size = chunksize (p);
nextchunk = chunk_at_offset(p, size);
nextsize = chunksize(nextchunk);

if (!prev_inuse(p)) {
prevsize = prev_size (p);
size += prevsize;
p = chunk_at_offset(p, -((long) prevsize));
if (__glibc_unlikely (chunksize(p) != prevsize))
malloc_printerr ("corrupted size vs. prev_size in fastbins");
unlink_chunk (av, p);
}

if (nextchunk != av->top) {
nextinuse = inuse_bit_at_offset(nextchunk, nextsize);

if (!nextinuse) {
size += nextsize;
unlink_chunk (av, nextchunk);
} else
clear_inuse_bit_at_offset(nextchunk, 0);

first_unsorted = unsorted_bin->fd;
unsorted_bin->fd = p;
first_unsorted->bk = p;

if (!in_smallbin_range (size)) {
p->fd_nextsize = NULL;
p->bk_nextsize = NULL;
}

set_head(p, size | PREV_INUSE);
p->bk = unsorted_bin;
p->fd = first_unsorted;
set_foot(p, size);
}

else {
size += nextsize;
set_head(p, size | PREV_INUSE);
av->top = p;
}

} while ( (p = nextp) != 0);

}
} while (fb++ != maxfb);
}
```
</details>

### рдЕрдирдХреНрд░рдорд┐рдд рдмрд┐рди

рдпрд╣ рд╕рдордп рд╣реИ рдЕрдирдХреНрд░рдорд┐рдд рдмрд┐рди рдХреА рдЬрд╛рдВрдЪ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдПрдХ рд╕рдВрднрд╛рд╡рд┐рдд рд╡реИрдз рдЪрдВрдХ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдПред

#### рдкреНрд░рд╛рд░рдВрдн

рдпрд╣ рдПрдХ рдмрдбрд╝реЗ рдлреЙрд░ рд▓реБрдХ рдХреЗ рд╕рд╛рде рд╢реБрд░реВ рд╣реЛрддрд╛ рд╣реИ рдЬреЛ рдЕрдирдХреНрд░рдорд┐рдд рдмрд┐рди рдХреЛ `bk` рджрд┐рд╢рд╛ рдореЗрдВ рдШреВрдореЗрдЧрд╛ рдЬрдм рддрдХ рдпрд╣ рд╕рдорд╛рдкреНрдд рдирд╣реАрдВ рд╣реЛ рдЬрд╛рддрд╛ рд╣реИ (рдПрд░реАрдирд╛ рд╕рдВрд░рдЪрдирд╛ рдХреЗ рд╕рд╛рде) `while ((victim = unsorted_chunks (av)->bk) != unsorted_chunks (av))`&#x20;

рдЗрд╕рдХреЗ рдЕрддрд┐рд░рд┐рдХреНрдд, рдХреБрдЫ рд╕реБрд░рдХреНрд╖рд╛ рдЬрд╛рдВрдЪ рд╣рд░ рдмрд╛рд░ рдХреА рдЬрд╛рддреА рд╣реИ рдЬрдм рдПрдХ рдирдпрд╛ рдЪрдВрдХ рд╡рд┐рдЪрд╛рд░ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ:

* рдпрджрд┐ рдЪрдВрдХ рдХрд╛ рдЖрдХрд╛рд░ рдЕрдЬреАрдм рд╣реИ (рдмрд╣реБрдд рдЫреЛрдЯрд╛ рдпрд╛ рдмрд╣реБрдд рдмрдбрд╝рд╛): `malloc(): invalid size (unsorted)`
* рдпрджрд┐ рдЕрдЧрд▓реЗ рдЪрдВрдХ рдХрд╛ рдЖрдХрд╛рд░ рдЕрдЬреАрдм рд╣реИ (рдмрд╣реБрдд рдЫреЛрдЯрд╛ рдпрд╛ рдмрд╣реБрдд рдмрдбрд╝рд╛): `malloc(): invalid next size (unsorted)`
* рдпрджрд┐ рдкрд┐рдЫрд▓реЗ рдЪрдВрдХ рджреНрд╡рд╛рд░рд╛ рд╕реВрдЪрд┐рдд рдЖрдХрд╛рд░ рдЪрдВрдХ рдХреЗ рдЖрдХрд╛рд░ рд╕реЗ рднрд┐рдиреНрди рд╣реИ: `malloc(): mismatching next->prev_size (unsorted)`
* рдпрджрд┐ рдирд╣реАрдВ `victim->bck->fd == victim` рдпрд╛ рдирд╣реАрдВ `victim->fd == av` (рдПрд░реАрдирд╛): `malloc(): unsorted double linked list corrupted`
* рд╣рдо рд╣рдореЗрд╢рд╛ рдЕрдВрддрд┐рдо рдЪреЗрдХ рдХрд░ рд░рд╣реЗ рд╣реИрдВ, рдЗрд╕рдХрд╛ `fd` рд╣рдореЗрд╢рд╛ рдПрд░реАрдирд╛ рд╕рдВрд░рдЪрдирд╛ рдХреА рдУрд░ рдЗрд╢рд╛рд░рд╛ рдХрд░рдирд╛ рдЪрд╛рд╣рд┐рдПред
* рдпрджрд┐ рдЕрдЧрд▓рд╛ рдЪрдВрдХ рд╕реВрдЪрд┐рдд рдирд╣реАрдВ рдХрд░ рд░рд╣рд╛ рд╣реИ рдХрд┐ рдкрд┐рдЫрд▓рд╛ рдЙрдкрдпреЛрдЧ рдореЗрдВ рд╣реИ: `malloc(): invalid next->prev_inuse (unsorted)`

<details>

<summary><code>_int_malloc</code> рдЕрдирдХреНрд░рдорд┐рдд рдмрд┐рди рдкреНрд░рд╛рд░рдВрдн</summary>
```c
/*
Process recently freed or remaindered chunks, taking one only if
it is exact fit, or, if this a small request, the chunk is remainder from
the most recent non-exact fit.  Place other traversed chunks in
bins.  Note that this step is the only place in any routine where
chunks are placed in bins.

The outer loop here is needed because we might not realize until
near the end of malloc that we should have consolidated, so must
do so and retry. This happens at most once, and only when we would
otherwise need to expand memory to service a "small" request.
*/

#if USE_TCACHE
INTERNAL_SIZE_T tcache_nb = 0;
size_t tc_idx = csize2tidx (nb);
if (tcache != NULL && tc_idx < mp_.tcache_bins)
tcache_nb = nb;
int return_cached = 0;

tcache_unsorted_count = 0;
#endif

for (;; )
{
int iters = 0;
while ((victim = unsorted_chunks (av)->bk) != unsorted_chunks (av))
{
bck = victim->bk;
size = chunksize (victim);
mchunkptr next = chunk_at_offset (victim, size);

if (__glibc_unlikely (size <= CHUNK_HDR_SZ)
|| __glibc_unlikely (size > av->system_mem))
malloc_printerr ("malloc(): invalid size (unsorted)");
if (__glibc_unlikely (chunksize_nomask (next) < CHUNK_HDR_SZ)
|| __glibc_unlikely (chunksize_nomask (next) > av->system_mem))
malloc_printerr ("malloc(): invalid next size (unsorted)");
if (__glibc_unlikely ((prev_size (next) & ~(SIZE_BITS)) != size))
malloc_printerr ("malloc(): mismatching next->prev_size (unsorted)");
if (__glibc_unlikely (bck->fd != victim)
|| __glibc_unlikely (victim->fd != unsorted_chunks (av)))
malloc_printerr ("malloc(): unsorted double linked list corrupted");
if (__glibc_unlikely (prev_inuse (next)))
malloc_printerr ("malloc(): invalid next->prev_inuse (unsorted)");

```
</details>

#### рдпрджрд┐ `in_smallbin_range`

рдпрджрд┐ рдЪрдВрдХ рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рд╕реЗ рдмрдбрд╝рд╛ рд╣реИ рддреЛ рдЗрд╕рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ, рдФрд░ рдЪрдВрдХ рд╕реНрдерд╛рди рдХрд╛ рд╢реЗрд╖ рдЕрдВрд╢ рдЕрд╡реНрдпрд╡рд╕реНрдерд┐рдд рд╕реВрдЪреА рдореЗрдВ рдбрд╛рд▓реЗрдВ рдФрд░ `last_remainder` рдХреЛ рдЗрд╕рдХреЗ рд╕рд╛рде рдЕрдкрдбреЗрдЯ рдХрд░реЗрдВред

<details>

<summary><code>_int_malloc</code> рдЕрд╡реНрдпрд╡рд╕реНрдерд┐рдд рдмрд┐рди <code>in_smallbin_range</code></summary>
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c#L4090C11-L4124C14

/*
If a small request, try to use last remainder if it is the
only chunk in unsorted bin.  This helps promote locality for
runs of consecutive small requests. This is the only
exception to best-fit, and applies only when there is
no exact fit for a small chunk.
*/

if (in_smallbin_range (nb) &&
bck == unsorted_chunks (av) &&
victim == av->last_remainder &&
(unsigned long) (size) > (unsigned long) (nb + MINSIZE))
{
/* split and reattach remainder */
remainder_size = size - nb;
remainder = chunk_at_offset (victim, nb);
unsorted_chunks (av)->bk = unsorted_chunks (av)->fd = remainder;
av->last_remainder = remainder;
remainder->bk = remainder->fd = unsorted_chunks (av);
if (!in_smallbin_range (remainder_size))
{
remainder->fd_nextsize = NULL;
remainder->bk_nextsize = NULL;
}

set_head (victim, nb | PREV_INUSE |
(av != &main_arena ? NON_MAIN_ARENA : 0));
set_head (remainder, remainder_size | PREV_INUSE);
set_foot (remainder, remainder_size);

check_malloced_chunk (av, victim, nb);
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}

```
</details>

рдпрджрд┐ рдпрд╣ рд╕рдлрд▓ рд░рд╣рд╛, рддреЛ рдЯреБрдХрдбрд╝рд╛ рдФрд░ рдЦрддреНрдо рд╣реЛ рдЧрдпрд╛, рдЕрдЧрд░ рдирд╣реАрдВ, рддреЛ рдХрд╛рд░реНрдп рдХреЛ рдЬрд╛рд░реА рд░рдЦреЗрдВ...

#### рдпрджрд┐ рдмрд░рд╛рдмрд░ рдЖрдХрд╛рд░ рд╣реИ

рдЪрдВрдХ рдХреЛ рдмрд┐рди рд╕реЗ рд╣рдЯрд╛рдирд╛ рдЬрд╛рд░реА рд░рдЦреЗрдВ, рдорд╛рдорд▓реЗ рдореЗрдВ рдЕрдЧрд░ рдЪрдВрдХ рдХрд╛ рдЖрд╡рд╢реНрдпрдХ рдЖрдХрд╛рд░ рдмрд┐рд▓реНрдХреБрд▓ рдЙрд╕реА рд╣реИ:

* рдпрджрд┐ рдЯреАрдХреИрд╢ рднрд░рд╛ рдирд╣реАрдВ рд╣реИ, рддреЛ рдЗрд╕реЗ рдЯреАрдХреИрд╢ рдореЗрдВ рдЬреЛрдбрд╝реЗрдВ рдФрд░ рдЬрд╛рд░реА рд░рдЦреЗрдВ рдХрд┐ рдПрдХ рдЯреАрдХреИрд╢ рдЪрдВрдХ рд╣реИ рдЬреЛ рдкреНрд░рдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ
* рдпрджрд┐ рдЯреАрдХреИрд╢ рднрд░рд╛ рд╣реИ, рддреЛ рдЗрд╕реЗ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ рдФрд░ рдЗрд╕реЗ рд╡рд╛рдкрд╕ рдХрд░реЗрдВред

<details>

<summary><code>_int_malloc</code> рдЕрдирд╕реЙрд░реНрдЯреЗрдб рдмрд┐рди рдмрд░рд╛рдмрд░ рдЖрдХрд╛рд░</summary>
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c#L4126C11-L4157C14

/* remove from unsorted list */
unsorted_chunks (av)->bk = bck;
bck->fd = unsorted_chunks (av);

/* Take now instead of binning if exact fit */

if (size == nb)
{
set_inuse_bit_at_offset (victim, size);
if (av != &main_arena)
set_non_main_arena (victim);
#if USE_TCACHE
/* Fill cache first, return to user only if cache fills.
We may return one of these chunks later.  */
if (tcache_nb > 0
&& tcache->counts[tc_idx] < mp_.tcache_count)
{
tcache_put (victim, tc_idx);
return_cached = 1;
continue;
}
else
{
#endif
check_malloced_chunk (av, victim, nb);
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
#if USE_TCACHE
}
#endif
}

```
</details>

рдпрджрд┐ рдЪрдВрдХ рд╡рд╛рдкрд╕ рдирд╣реАрдВ рд▓реМрдЯрд╛рдпрд╛ рдЧрдпрд╛ рдпрд╛ рдЯреАрдХреИрдЪ рдореЗрдВ рдЬреЛрдбрд╝рд╛ рдирд╣реАрдВ рдЧрдпрд╛ рд╣реИ, рдХреЛрдб рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рд░рдЦреЗрдВ...

#### рдмрд┐рди рдореЗрдВ рдЪрдВрдХ рд░рдЦреЗрдВ

рдЪреЗрдХ рдХрд┐рдП рдЧрдП рдЪрдВрдХ рдХреЛ рдЫреЛрдЯреЗ рдмрд┐рди рдореЗрдВ рдпрд╛ рдмрдбрд╝реЗ рдмрд┐рди рдореЗрдВ рд░рдЦреЗрдВ рдЪрдВрдХ рдХреЗ рдЖрдХрд╛рд░ рдХреЗ рдЕрдиреБрд╕рд╛рд░ (рдмрдбрд╝реЗ рдмрд┐рди рдХреЛ рдареАрдХ рд╕реЗ рд╕рдВрдЧрдард┐рдд рд░рдЦрддреЗ рд╣реБрдП)ред

рдпрд╣рд╛рдБ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рд╕реБрд░рдХреНрд╖рд╛ рдЬрд╛рдВрдЪреЗрдВ рдХреА рдЬрд╛ рд░рд╣реА рд╣реИрдВ рдХрд┐ рдХреНрдпрд╛ рдмрдбрд╝реЗ рдмрд┐рди рдбрдмрд▓ рд▓рд┐рдВрдХреНрдб рд╕реВрдЪреА рдХреЛ рдХреЛрд░рдкреНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИ:

* рдпрджрд┐ `fwd->bk_nextsize->fd_nextsize != fwd`: `malloc(): largebin double linked list corrupted (nextsize)`
* рдпрджрд┐ `fwd->bk->fd != fwd`: `malloc(): largebin double linked list corrupted (bk)`

<details>

<summary><code>_int_malloc</code> рдмрд┐рди рдореЗрдВ рдЪрдВрдХ рд░рдЦреЗрдВ</summary>
```c
/* place chunk in bin */

if (in_smallbin_range (size))
{
victim_index = smallbin_index (size);
bck = bin_at (av, victim_index);
fwd = bck->fd;
}
else
{
victim_index = largebin_index (size);
bck = bin_at (av, victim_index);
fwd = bck->fd;

/* maintain large bins in sorted order */
if (fwd != bck)
{
/* Or with inuse bit to speed comparisons */
size |= PREV_INUSE;
/* if smaller than smallest, bypass loop below */
assert (chunk_main_arena (bck->bk));
if ((unsigned long) (size)
< (unsigned long) chunksize_nomask (bck->bk))
{
fwd = bck;
bck = bck->bk;

victim->fd_nextsize = fwd->fd;
victim->bk_nextsize = fwd->fd->bk_nextsize;
fwd->fd->bk_nextsize = victim->bk_nextsize->fd_nextsize = victim;
}
else
{
assert (chunk_main_arena (fwd));
while ((unsigned long) size < chunksize_nomask (fwd))
{
fwd = fwd->fd_nextsize;
assert (chunk_main_arena (fwd));
}

if ((unsigned long) size
== (unsigned long) chunksize_nomask (fwd))
/* Always insert in the second position.  */
fwd = fwd->fd;
else
{
victim->fd_nextsize = fwd;
victim->bk_nextsize = fwd->bk_nextsize;
if (__glibc_unlikely (fwd->bk_nextsize->fd_nextsize != fwd))
malloc_printerr ("malloc(): largebin double linked list corrupted (nextsize)");
fwd->bk_nextsize = victim;
victim->bk_nextsize->fd_nextsize = victim;
}
bck = fwd->bk;
if (bck->fd != fwd)
malloc_printerr ("malloc(): largebin double linked list corrupted (bk)");
}
}
else
victim->fd_nextsize = victim->bk_nextsize = victim;
}

mark_bin (av, victim_index);
victim->bk = bck;
victim->fd = fwd;
fwd->bk = victim;
bck->fd = victim;
```
</details>

#### `_int_malloc` рд╕реАрдорд╛рдПрдБ

рдЗрд╕ рдмрд┐рдВрджреБ рдкрд░, рдпрджрд┐ рдХреБрдЫ рдЪрдВрдХ tcache рдореЗрдВ рд╕рдВрдЧреНрд░рд╣рд┐рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рдерд╛ рдЬреЛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ рдФрд░ рд╕реАрдорд╛ рдкрд╣реБрдВрдЪ рдЧрдИ рд╣реИ, рддреЛ **рдПрдХ tcache рдЪрдВрдХ рд╡рд╛рдкрд╕ рд▓реМрдЯрд╛рдПрдВ**ред

рдЗрд╕рдХреЗ рдЕрддрд┐рд░рд┐рдХреНрдд, рдпрджрд┐ **MAX\_ITERS** рдкрд╣реБрдВрдЪ рдЧрдпрд╛ рд╣реИ, рддреЛ рд▓реВрдк рд╕реЗ рдмрд╛рд╣рд░ рдирд┐рдХрд▓реЗрдВ рдФрд░ рдПрдХ рдЕрд▓рдЧ рддрд░реАрдХреЗ рд╕реЗ рдЪрдВрдХ рдкреНрд░рд╛рдкреНрдд рдХрд░реЗрдВ (рд╢реАрд░реНрд╖ рдЪрдВрдХ)ред

рдпрджрд┐ `return_cached` рд╕реЗрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ рдерд╛, рддреЛ рдмрдбрд╝реА рдЦреЛрдЬреЛрдВ рд╕реЗ рдмрдЪрдиреЗ рдХреЗ рд▓рд┐рдП рдмрд╕ tcache рд╕реЗ рдПрдХ рдЪрдВрдХ рд╡рд╛рдкрд╕ рд▓реМрдЯрд╛рдПрдВред

<details>

<summary><code>_int_malloc</code> рд╕реАрдорд╛рдПрдБ</summary>
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c#L4227C1-L4250C7

#if USE_TCACHE
/* If we've processed as many chunks as we're allowed while
filling the cache, return one of the cached ones.  */
++tcache_unsorted_count;
if (return_cached
&& mp_.tcache_unsorted_limit > 0
&& tcache_unsorted_count > mp_.tcache_unsorted_limit)
{
return tcache_get (tc_idx);
}
#endif

#define MAX_ITERS       10000
if (++iters >= MAX_ITERS)
break;
}

#if USE_TCACHE
/* If all the small chunks we found ended up cached, return one now.  */
if (return_cached)
{
return tcache_get (tc_idx);
}
#endif
```
</details>

рдпрджрд┐ рд╕реАрдорд╛рдПрдБ рдирд╣реАрдВ рдкрд╣реБрдВрдЪреА рд╣реИрдВ, рддреЛ рдХреЛрдб рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рд░рдЦреЗрдВ...

### рдмрдбрд╝рд╛ рдмрд┐рди (рдЗрдВрдбреЗрдХреНрд╕ рджреНрд╡рд╛рд░рд╛)

рдпрджрд┐ рдЕрдиреБрд░реЛрдз рдмрдбрд╝рд╛ рд╣реИ (рдЫреЛрдЯреЗ рдмрд┐рди рдореЗрдВ рдирд╣реАрдВ рд╣реИ) рдФрд░ рд╣рдордиреЗ рдЕрдм рддрдХ рдХреЛрдИ рднреА рдЯреБрдХрдбрд╝рд╛ рд╡рд╛рдкрд╕ рдирд╣реАрдВ рдХрд┐рдпрд╛ рд╣реИ, рддреЛ **рдмрдбрд╝реЗ рдмрд┐рди** рдореЗрдВ рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рдХрд╛ **рдЗрдВрдбреЗрдХреНрд╕** рдкреНрд░рд╛рдкреНрдд рдХрд░реЗрдВ, рдЬрд╛рдВрдЪреЗрдВ рдХрд┐ рдХреНрдпрд╛ рдпрд╣ **рдЦрд╛рд▓реА рдирд╣реАрдВ рд╣реИ** рдпрд╛ рдХреНрдпрд╛ **рдЗрд╕ рдмрд┐рди рдореЗрдВ рд╕рдмрд╕реЗ рдмрдбрд╝рд╛ рдЯреБрдХрдбрд╝рд╛ рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рд╕реЗ рдмрдбрд╝рд╛ рд╣реИ** рдФрд░ рдЙрд╕ рдорд╛рдорд▓реЗ рдореЗрдВ **рдЙрд╕ рдЫреЛрдЯреЗ рдЯреБрдХрдбрд╝реЗ рдХреЛ рдЦреЛрдЬреЗрдВ рдЬреЛ рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ**ред

рдЕрдВрдд рдореЗрдВ рдЙрдкрдпреЛрдЧ рдХрд┐рдП рдЧрдП рдЯреБрдХрдбрд╝реЗ рд╕реЗ рд╢реЗрд╖ рд╕реНрдерд╛рди рдПрдХ рдирдП рдЯреБрдХрдбрд╝реЗ рдХреЗ рд▓рд┐рдП рд╣реЛ рд╕рдХрддрд╛ рд╣реИ, рддреЛ рдЗрд╕реЗ рдЕрд╡реНрдпрд╡рд╕реНрдерд┐рдд рдмрд┐рди рдореЗрдВ рдЬреЛрдбрд╝реЗрдВ рдФрд░ lsast\_reminder рдЕрдкрдбреЗрдЯ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред

рдЬрдм рдЕрд╡реНрдпрд╡рд╕реНрдерд┐рдд рдмрд┐рди рдореЗрдВ рдпрд╛рджрджрд╛рд╢реНрдд рдЬреЛрдбрд╝реА рдЬрд╛рддреА рд╣реИ, рддреЛ рдПрдХ рд╕реБрд░рдХреНрд╖рд╛ рдЬрд╛рдВрдЪ рдХреА рдЬрд╛рддреА рд╣реИ:

* `bck->fd-> bk != bck`: `malloc(): corrupted unsorted chunks`

<details>

<summary><code>_int_malloc</code> рдмрдбрд╝рд╛ рдмрд┐рди (рдЗрдВрдбреЗрдХреНрд╕ рджреНрд╡рд╛рд░рд╛)</summary>
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c#L4252C7-L4317C10

/*
If a large request, scan through the chunks of current bin in
sorted order to find smallest that fits.  Use the skip list for this.
*/

if (!in_smallbin_range (nb))
{
bin = bin_at (av, idx);

/* skip scan if empty or largest chunk is too small */
if ((victim = first (bin)) != bin
&& (unsigned long) chunksize_nomask (victim)
>= (unsigned long) (nb))
{
victim = victim->bk_nextsize;
while (((unsigned long) (size = chunksize (victim)) <
(unsigned long) (nb)))
victim = victim->bk_nextsize;

/* Avoid removing the first entry for a size so that the skip
list does not have to be rerouted.  */
if (victim != last (bin)
&& chunksize_nomask (victim)
== chunksize_nomask (victim->fd))
victim = victim->fd;

remainder_size = size - nb;
unlink_chunk (av, victim);

/* Exhaust */
if (remainder_size < MINSIZE)
{
set_inuse_bit_at_offset (victim, size);
if (av != &main_arena)
set_non_main_arena (victim);
}
/* Split */
else
{
remainder = chunk_at_offset (victim, nb);
/* We cannot assume the unsorted list is empty and therefore
have to perform a complete insert here.  */
bck = unsorted_chunks (av);
fwd = bck->fd;
if (__glibc_unlikely (fwd->bk != bck))
malloc_printerr ("malloc(): corrupted unsorted chunks");
last_re->bk = bck;
remainder->fd = fwd;
bck->fd = remainder;
fwd->bk = remainder;
if (!in_smallbin_range (remainder_size))
{
remainder->fd_nextsize = NULL;
remainder->bk_nextsize = NULL;
}
set_head (victim, nb | PREV_INUSE |
(av != &main_arena ? NON_MAIN_ARENA : 0));
set_head (remainder, remainder_size | PREV_INUSE);
set_foot (remainder, remainder_size);
}
check_malloced_chunk (av, victim, nb);
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}
}
```
</details>

рдпрджрд┐ рдХреЛрдИ рдЪрдВрдХ рдЗрд╕рдХреЗ рд▓рд┐рдП рдЙрдкрдпреБрдХреНрдд рдирд╣реАрдВ рдорд┐рд▓рддрд╛ рд╣реИ, рддреЛ рдЬрд╛рд░реА рд░рдЦреЗрдВ

### рдмрдбрд╝рд╛ рдмрд┐рди (рдЕрдЧрд▓рд╛ рдмрдбрд╝рд╛)

рдпрджрд┐ рд╕рдЯреАрдХ рдмрдбрд╝реЗ рдмрд┐рди рдореЗрдВ рдХреЛрдИ рдЪрдВрдХ рдирд╣реАрдВ рдерд╛ рдЬрд┐рд╕рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рдерд╛, рддреЛ рд╕рднреА рдЕрдЧрд▓реЗ рдмрдбрд╝реЗ рдмрд┐рди рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рд▓реВрдкрд┐рдВрдЧ рд╢реБрд░реВ рдХрд░реЗрдВ (рдЬреЛ рддреБрд░рдВрдд рдмрдбрд╝рд╛ рд╣реИ) рдЬрдм рддрдХ рдХреЛрдИ рдПрдХ рдирд╣реАрдВ рдорд┐рд▓рддрд╛ рд╣реИ (рдпрджрд┐ рдХреЛрдИ рд╣реЛ)

рд╡рд┐рднрд╛рдЬрд┐рдд рдЪрдВрдХ рдХрд╛ рд╢реЗрд╖рд╛рдВрд╢ рдЕрд╕рдВрдЧрдард┐рдд рдмрд┐рди рдореЗрдВ рдЬреЛрдбрд╝рд╛ рдЬрд╛рддрд╛ рд╣реИ, рдЕрдВрддрд┐рдо рд╢реЗрд╖рд╛рдВрд╢ рдЕрдкрдбреЗрдЯ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ рдФрд░ рд╕рдорд╛рди рд╕реБрд░рдХреНрд╖рд╛ рдЬрд╛рдВрдЪ рдХреА рдЬрд╛рддреА рд╣реИ:

* `bck->fd-> bk != bck`: `malloc(): corrupted unsorted chunks2`

<details>

<summary><code>_int_malloc</code> рдмрдбрд╝рд╛ рдмрд┐рди (рдЕрдЧрд▓рд╛ рдмрдбрд╝рд╛)</summary>
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c#L4319C7-L4425C10

/*
Search for a chunk by scanning bins, starting with next largest
bin. This search is strictly by best-fit; i.e., the smallest
(with ties going to approximately the least recently used) chunk
that fits is selected.

The bitmap avoids needing to check that most blocks are nonempty.
The particular case of skipping all bins during warm-up phases
when no chunks have been returned yet is faster than it might look.
*/

++idx;
bin = bin_at (av, idx);
block = idx2block (idx);
map = av->binmap[block];
bit = idx2bit (idx);

for (;; )
{
/* Skip rest of block if there are no more set bits in this block.  */
if (bit > map || bit == 0)
{
do
{
if (++block >= BINMAPSIZE) /* out of bins */
goto use_top;
}
while ((map = av->binmap[block]) == 0);

bin = bin_at (av, (block << BINMAPSHIFT));
bit = 1;
}

/* Advance to bin with set bit. There must be one. */
while ((bit & map) == 0)
{
bin = next_bin (bin);
bit <<= 1;
assert (bit != 0);
}

/* Inspect the bin. It is likely to be non-empty */
victim = last (bin);

/*  If a false alarm (empty bin), clear the bit. */
if (victim == bin)
{
av->binmap[block] = map &= ~bit; /* Write through */
bin = next_bin (bin);
bit <<= 1;
}

else
{
size = chunksize (victim);

/*  We know the first chunk in this bin is big enough to use. */
assert ((unsigned long) (size) >= (unsigned long) (nb));

remainder_size = size - nb;

/* unlink */
unlink_chunk (av, victim);

/* Exhaust */
if (remainder_size < MINSIZE)
{
set_inuse_bit_at_offset (victim, size);
if (av != &main_arena)
set_non_main_arena (victim);
}

/* Split */
else
{
remainder = chunk_at_offset (victim, nb);

/* We cannot assume the unsorted list is empty and therefore
have to perform a complete insert here.  */
bck = unsorted_chunks (av);
fwd = bck->fd;
if (__glibc_unlikely (fwd->bk != bck))
malloc_printerr ("malloc(): corrupted unsorted chunks 2");
remainder->bk = bck;
remainder->fd = fwd;
bck->fd = remainder;
fwd->bk = remainder;

/* advertise as last remainder */
if (in_smallbin_range (nb))
av->last_remainder = remainder;
if (!in_smallbin_range (remainder_size))
{
remainder->fd_nextsize = NULL;
remainder->bk_nextsize = NULL;
}
set_head (victim, nb | PREV_INUSE |
(av != &main_arena ? NON_MAIN_ARENA : 0));
set_head (remainder, remainder_size | PREV_INUSE);
set_foot (remainder, remainder_size);
}
check_malloced_chunk (av, victim, nb);
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}
}
```
</details>

### рд╢реАрд░реНрд╖ рдЯреБрдХрдбрд╝рд╛

рдЗрд╕ рдмрд┐рдВрджреБ рдкрд░, рдирдП рдЪрдВрдХ рдХреЛ рд╢реАрд░реНрд╖ рдЪрдВрдХ рд╕реЗ рдкреНрд░рд╛рдкреНрдд рдХрд░рдиреЗ рдХрд╛ рд╕рдордп рд╣реИ (рдЕрдЧрд░ рдкрд░реНрдпрд╛рдкреНрдд рдмрдбрд╝рд╛ рд╣реИ)ред

рдпрд╣ рдПрдХ рд╕реБрд░рдХреНрд╖рд╛ рдЬрд╛рдВрдЪ рдХреЗ рд╕рд╛рде рд╢реБрд░реВ рд╣реЛрддрд╛ рд╣реИ рдЬрд┐рд╕реЗ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рдирд╛ рд╣реИ рдХрд┐ рдЪрдВрдХ рдХрд╛ рдЖрдХрд╛рд░ рдмрд╣реБрдд рдмрдбрд╝рд╛ рдирд╣реАрдВ рд╣реИ (рдХреНрд╖рддрд┐рдЧреНрд░рд╕реНрдд):

* `chunksize(av->top) > av->system_mem`: `malloc(): corrupted top size`

рдлрд┐рд░, рдпрд╣ рд╢реАрд░реНрд╖ рдЪрдВрдХ рд╕реНрдерд╛рди рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдЧрд╛ рдЕрдЧрд░ рдпрд╣ рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рдХрд╛ рдЪрдВрдХ рдмрдирд╛рдиреЗ рдХреЗ рд▓рд┐рдП рдкрд░реНрдпрд╛рдкреНрдд рдмрдбрд╝рд╛ рд╣реИред\
рдЕрдЧрд░ рдирд╣реАрдВ, рдЕрдЧрд░ рддреЗрдЬ рдЪрдВрдХ рд╣реИрдВ, рддреЛ рдЙрдиреНрд╣реЗрдВ рд╕рдореЗрдХрд┐рдд рдХрд░реЗрдВ рдФрд░ рдлрд┐рд░ рд╕реЗ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВред\
рдЕрдВрддрддрдГ, рдЕрдЧрд░ рдкрд░реНрдпрд╛рдкреНрдд рд╕реНрдерд╛рди рдирд╣реАрдВ рд╣реИ рддреЛ `sysmalloc` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ рддрд╛рдХрд┐ рдкрд░реНрдпрд╛рдкреНрдд рдЖрдХрд╛рд░ рдХрд╛ рдЖрд╡рдВрдЯрд┐рдд рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХреЗред

<details>

<summary><code>_int_malloc</code> рд╢реАрд░реНрд╖ рдЯреБрдХрдбрд╝рд╛</summary>
```c
use_top:
/*
If large enough, split off the chunk bordering the end of memory
(held in av->top). Note that this is in accord with the best-fit
search rule.  In effect, av->top is treated as larger (and thus
less well fitting) than any other available chunk since it can
be extended to be as large as necessary (up to system
limitations).

We require that av->top always exists (i.e., has size >=
MINSIZE) after initialization, so if it would otherwise be
exhausted by current request, it is replenished. (The main
reason for ensuring it exists is that we may need MINSIZE space
to put in fenceposts in sysmalloc.)
*/

victim = av->top;
size = chunksize (victim);

if (__glibc_unlikely (size > av->system_mem))
malloc_printerr ("malloc(): corrupted top size");

if ((unsigned long) (size) >= (unsigned long) (nb + MINSIZE))
{
remainder_size = size - nb;
remainder = chunk_at_offset (victim, nb);
av->top = remainder;
set_head (victim, nb | PREV_INUSE |
(av != &main_arena ? NON_MAIN_ARENA : 0));
set_head (remainder, remainder_size | PREV_INUSE);

check_malloced_chunk (av, victim, nb);
void *p = chunk2mem (victim);
alloc_perturb (p, bytes);
return p;
}

/* When we are using atomic ops to free fast chunks we can get
here for all block sizes.  */
else if (atomic_load_relaxed (&av->have_fastchunks))
{
malloc_consolidate (av);
/* restore original bin index */
if (in_smallbin_range (nb))
idx = smallbin_index (nb);
else
idx = largebin_index (nb);
}

/*
Otherwise, relay to handle system-dependent cases
*/
else
{
void *p = sysmalloc (nb, av);
if (p != NULL)
alloc_perturb (p, bytes);
return p;
}
}
}

```
</details>

## sysmalloc

### рд╕рд┐рд╕рдореИрд▓реЛрдХ рд╢реБрд░реВ

рдпрджрд┐ рдПрд░реАрдирд╛ рдирд▓ рд╣реИ рдпрд╛ рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рдмрд╣реБрдд рдмрдбрд╝рд╛ рд╣реИ (рдФрд░ рдЕрдиреБрдорддрд┐ рдмрдЪреА рд╣реБрдИ mmaps рд╣реИрдВ) рддреЛ рд╕реНрдерд╛рди рдЖрд╡рдВрдЯрд┐рдд рдХрд░рдиреЗ рдФрд░ рдЗрд╕реЗ рд▓реМрдЯрд╛рдиреЗ рдХреЗ рд▓рд┐рдП `sysmalloc_mmap` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВред

<details>

<summary>рд╕рд┐рд╕рдореИрд▓реЛрдХ рд╢реБрд░реВ</summary>
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2531

/*
sysmalloc handles malloc cases requiring more memory from the system.
On entry, it is assumed that av->top does not have enough
space to service request for nb bytes, thus requiring that av->top
be extended or replaced.
*/

static void *
sysmalloc (INTERNAL_SIZE_T nb, mstate av)
{
mchunkptr old_top;              /* incoming value of av->top */
INTERNAL_SIZE_T old_size;       /* its size */
char *old_end;                  /* its end address */

long size;                      /* arg to first MORECORE or mmap call */
char *brk;                      /* return value from MORECORE */

long correction;                /* arg to 2nd MORECORE call */
char *snd_brk;                  /* 2nd return val */

INTERNAL_SIZE_T front_misalign; /* unusable bytes at front of new space */
INTERNAL_SIZE_T end_misalign;   /* partial page left at end of new space */
char *aligned_brk;              /* aligned offset into brk */

mchunkptr p;                    /* the allocated/returned chunk */
mchunkptr remainder;            /* remainder from allocation */
unsigned long remainder_size;   /* its size */


size_t pagesize = GLRO (dl_pagesize);
bool tried_mmap = false;


/*
If have mmap, and the request size meets the mmap threshold, and
the system supports mmap, and there are few enough currently
allocated mmapped regions, try to directly map this request
rather than expanding top.
*/

if (av == NULL
|| ((unsigned long) (nb) >= (unsigned long) (mp_.mmap_threshold)
&& (mp_.n_mmaps < mp_.n_mmaps_max)))
{
char *mm;
if (mp_.hp_pagesize > 0 && nb >= mp_.hp_pagesize)
{
/* There is no need to issue the THP madvise call if Huge Pages are
used directly.  */
mm = sysmalloc_mmap (nb, mp_.hp_pagesize, mp_.hp_flags, av);
if (mm != MAP_FAILED)
return mm;
}
mm = sysmalloc_mmap (nb, pagesize, 0, av);
if (mm != MAP_FAILED)
return mm;
tried_mmap = true;
}

/* There are no usable arenas and mmap also failed.  */
if (av == NULL)
return 0;
```
</details>

### рд╕рд┐рд╕рдореИрд▓реЛрдХ рдЬрд╛рдВрдЪреЗрдВ

рдпрд╣ рдкреБрд░рд╛рдиреЗ рдЯреЙрдк рдЪрдВрдХ рдЬрд╛рдирдХрд╛рд░реА рдкреНрд░рд╛рдкреНрдд рдХрд░рдХреЗ рд╢реБрд░реВ рд╣реЛрддрд╛ рд╣реИ рдФрд░ рдпрд╣ рдЬрд╛рдВрдЪрддрд╛ рд╣реИ рдХрд┐ рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рд╢рд░реНрддреЛрдВ рдореЗрдВ рд╕реЗ рдХреБрдЫ рд╕рддреНрдп рд╣реИрдВ:

* рдкреБрд░рд╛рдирд╛ рд╣реАрдк рдХрд╛ рдЖрдХрд╛рд░ 0 рд╣реИ (рдирдпрд╛ рд╣реАрдк)
* рдкрд┐рдЫрд▓реЗ рд╣реАрдк рдХрд╛ рдЖрдХрд╛рд░ MINSIZE рд╕реЗ рдЕрдзрд┐рдХ рд╣реИ рдФрд░ рдкреБрд░рд╛рдирд╛ рдЯреЙрдк рдЙрдкрдпреЛрдЧ рдореЗрдВ рд╣реИ
* рд╣реАрдк рдкреЗрдЬ рд╕рд╛рдЗрдЬ (0x1000) рдХреЗ рд▓рд┐рдП рдПрдХрд░реВрдк рд╣реИ (рд▓реЛрдЕрд░ 12 рдмрд┐рдЯ 0 рд╣реЛрдиреЗ рдЪрд╛рд╣рд┐рдП)

рдлрд┐рд░ рдпрд╣ рднреА рдЬрд╛рдВрдЪрддрд╛ рд╣реИ рдХрд┐:

* рдкреБрд░рд╛рдиреЗ рдЖрдХрд╛рд░ рдореЗрдВ рдкрд░реНрдпрд╛рдкреНрдд рд╕реНрдерд╛рди рдирд╣реАрдВ рд╣реИ рдЬрд┐рд╕рд╕реЗ рдЕрдиреБрд░реЛрдзрд┐рдд рдЖрдХрд╛рд░ рдХреЗ рд▓рд┐рдП рдПрдХ рдЪрдВрдХ рдмрдирд╛рдпрд╛ рдЬрд╛ рд╕рдХреЗ
```c
/* Record incoming configuration of top */

old_top = av->top;
old_size = chunksize (old_top);
old_end = (char *) (chunk_at_offset (old_top, old_size));

brk = snd_brk = (char *) (MORECORE_FAILURE);

/*
If not the first time through, we require old_size to be
at least MINSIZE and to have prev_inuse set.
*/

assert ((old_top == initial_top (av) && old_size == 0) ||
((unsigned long) (old_size) >= MINSIZE &&
prev_inuse (old_top) &&
((unsigned long) old_end & (pagesize - 1)) == 0));

/* Precondition: not enough current space to satisfy nb request */
assert ((unsigned long) (old_size) < (unsigned long) (nb + MINSIZE));
```
</details>

### sysmalloc рдореБрдЦреНрдп рдПрд░рд┐рдирд╛ рдирд╣реАрдВ

рдпрд╣ рдкрд╣рд▓реЗ рдкрд┐рдЫрд▓реЗ рд╣реАрдк рдХреЛ рдмрдврд╝рд╛рдиреЗ рдХреА рдХреЛрд╢рд┐рд╢ рдХрд░реЗрдЧрд╛ред рдЕрдЧрд░ рдпрд╣ рд╕рдВрднрд╡ рдирд╣реАрдВ рд╣реИ рддреЛ рдПрдХ рдирдпрд╛ рд╣реАрдк рдЖрд╡рдВрдЯрд┐рдд рдХрд░рдиреЗ рдХреА рдХреЛрд╢рд┐рд╢ рдХрд░реЗрдЧрд╛ рдФрд░ рдЗрд╕рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдкреЙрдЗрдВрдЯрд░ рдХреЛ рдЕрдкрдбреЗрдЯ рдХрд░реЗрдЧрд╛ред\
рдЕрдВрдд рдореЗрдВ рдЕрдЧрд░ рдпрд╣ рдХрд╛рдо рдирд╣реАрдВ рдХрд░рддрд╛ рд╣реИ, рддреЛ **`sysmalloc_mmap`** рдХреЛ рдХреЙрд▓ рдХрд░рдиреЗ рдХреА рдХреЛрд╢рд┐рд╢ рдХрд░реЗрдВред&#x20;

<details>

<summary>sysmalloc рдореБрдЦреНрдп рдПрд░рд┐рдирд╛ рдирд╣реАрдВ</summary>
```c
if (av != &main_arena)
{
heap_info *old_heap, *heap;
size_t old_heap_size;

/* First try to extend the current heap. */
old_heap = heap_for_ptr (old_top);
old_heap_size = old_heap->size;
if ((long) (MINSIZE + nb - old_size) > 0
&& grow_heap (old_heap, MINSIZE + nb - old_size) == 0)
{
av->system_mem += old_heap->size - old_heap_size;
set_head (old_top, (((char *) old_heap + old_heap->size) - (char *) old_top)
| PREV_INUSE);
}
else if ((heap = new_heap (nb + (MINSIZE + sizeof (*heap)), mp_.top_pad)))
{
/* Use a newly allocated heap.  */
heap->ar_ptr = av;
heap->prev = old_heap;
av->system_mem += heap->size;
/* Set up the new top.  */
top (av) = chunk_at_offset (heap, sizeof (*heap));
set_head (top (av), (heap->size - sizeof (*heap)) | PREV_INUSE);

/* Setup fencepost and free the old top chunk with a multiple of
MALLOC_ALIGNMENT in size. */
/* The fencepost takes at least MINSIZE bytes, because it might
become the top chunk again later.  Note that a footer is set
up, too, although the chunk is marked in use. */
old_size = (old_size - MINSIZE) & ~MALLOC_ALIGN_MASK;
set_head (chunk_at_offset (old_top, old_size + CHUNK_HDR_SZ),
0 | PREV_INUSE);
if (old_size >= MINSIZE)
{
set_head (chunk_at_offset (old_top, old_size),
CHUNK_HDR_SZ | PREV_INUSE);
set_foot (chunk_at_offset (old_top, old_size), CHUNK_HDR_SZ);
set_head (old_top, old_size | PREV_INUSE | NON_MAIN_ARENA);
_int_free (av, old_top, 1);
}
else
{
set_head (old_top, (old_size + CHUNK_HDR_SZ) | PREV_INUSE);
set_foot (old_top, (old_size + CHUNK_HDR_SZ));
}
}
else if (!tried_mmap)
{
/* We can at least try to use to mmap memory.  If new_heap fails
it is unlikely that trying to allocate huge pages will
succeed.  */
char *mm = sysmalloc_mmap (nb, pagesize, 0, av);
if (mm != MAP_FAILED)
return mm;
}
}
```
</details>

### sysmalloc рдореБрдЦреНрдп рдПрд░реАрдирд╛

рдпрд╣ рдореЗрдореЛрд░реА рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдХреА рдЧрдгрдирд╛ рд╢реБрд░реВ рдХрд░рддрд╛ рд╣реИред рдпрд╣ рдПрдХ рд╕рд╛рдереА рдореЗрдореЛрд░реА рдХрд╛ рдЕрдиреБрд░реЛрдз рдХрд░рдХреЗ рд╢реБрд░реВ рд╣реЛрдЧрд╛ рддрд╛рдХрд┐ рдЗрд╕ рдорд╛рдорд▓реЗ рдореЗрдВ рдкреБрд░рд╛рдиреА рдореЗрдореЛрд░реА рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХреЗред рдЗрд╕рдХреЗ рдЕрд▓рд╛рд╡рд╛ рдХреБрдЫ рд╕рдордВрдЬрд┐рдд рдХрд╛рд░реНрдпреЛрдВ рдХреЛ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред

<details>

<summary>sysmalloc рдореБрдЦреНрдп рдПрд░реАрдирд╛</summary>
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2665C1-L2713C10

else     /* av == main_arena */


{ /* Request enough space for nb + pad + overhead */
size = nb + mp_.top_pad + MINSIZE;

/*
If contiguous, we can subtract out existing space that we hope to
combine with new space. We add it back later only if
we don't actually get contiguous space.
*/

if (contiguous (av))
size -= old_size;

/*
Round to a multiple of page size or huge page size.
If MORECORE is not contiguous, this ensures that we only call it
with whole-page arguments.  And if MORECORE is contiguous and
this is not first time through, this preserves page-alignment of
previous calls. Otherwise, we correct to page-align below.
*/

#ifdef MADV_HUGEPAGE
/* Defined in brk.c.  */
extern void *__curbrk;
if (__glibc_unlikely (mp_.thp_pagesize != 0))
{
uintptr_t top = ALIGN_UP ((uintptr_t) __curbrk + size,
mp_.thp_pagesize);
size = top - (uintptr_t) __curbrk;
}
else
#endif
size = ALIGN_UP (size, GLRO(dl_pagesize));

/*
Don't try to call MORECORE if argument is so big as to appear
negative. Note that since mmap takes size_t arg, it may succeed
below even if we cannot call MORECORE.
*/

if (size > 0)
{
brk = (char *) (MORECORE (size));
if (brk != (char *) (MORECORE_FAILURE))
madvise_thp (brk, size);
LIBC_PROBE (memory_sbrk_more, 2, brk, size);
}
```
### sysmalloc рдореБрдЦреНрдп рдПрд░реАрдирд╛ рдкрд┐рдЫрд▓рд╛ рддреНрд░реБрдЯрд┐ 1

рдпрджрд┐ рдкрд┐рдЫрд▓рд╛ `MORECORE_FAILURE` рд▓реМрдЯрд╛рдпрд╛ рдЧрдпрд╛ рд╣реИ, рддреЛ `sysmalloc_mmap_fallback` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ рдореЗрдореЛрд░реА рдХрд╛ рдЖрд╡рдВрдЯрди рдлрд┐рд░ рд╕реЗ рдХрд░рдиреЗ рдХрд╛ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВ
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2715C7-L2740C10

if (brk == (char *) (MORECORE_FAILURE))
{
/*
If have mmap, try using it as a backup when MORECORE fails or
cannot be used. This is worth doing on systems that have "holes" in
address space, so sbrk cannot extend to give contiguous space, but
space is available elsewhere.  Note that we ignore mmap max count
and threshold limits, since the space will not be used as a
segregated mmap region.
*/

char *mbrk = MAP_FAILED;
if (mp_.hp_pagesize > 0)
mbrk = sysmalloc_mmap_fallback (&size, nb, old_size,
mp_.hp_pagesize, mp_.hp_pagesize,
mp_.hp_flags, av);
if (mbrk == MAP_FAILED)
mbrk = sysmalloc_mmap_fallback (&size, nb, old_size, MMAP_AS_MORECORE_SIZE,
pagesize, 0, av);
if (mbrk != MAP_FAILED)
{
/* We do not need, and cannot use, another sbrk call to find end */
brk = mbrk;
snd_brk = brk + size;
}
}
```
### sysmalloc рдореБрдЦреНрдп рдПрд░реАрдирд╛ рдЬрд╛рд░реА рд░рдЦреЗрдВ

рдпрджрд┐ рдкрд┐рдЫрд▓рд╛ `MORECORE_FAILURE` рдирд╣реАрдВ рд▓реМрдЯрд╛рдпрд╛, рдЕрдЧрд░ рдпрд╣ рдХрд╛рдо рдХрд░рддрд╛ рд╣реИ рддреЛ рдХреБрдЫ рд╕рдВрд░реЗрдЦрдг рдмрдирд╛рдПрдВ:
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2742

if (brk != (char *) (MORECORE_FAILURE))
{
if (mp_.sbrk_base == 0)
mp_.sbrk_base = brk;
av->system_mem += size;

/*
If MORECORE extends previous space, we can likewise extend top size.
*/

if (brk == old_end && snd_brk == (char *) (MORECORE_FAILURE))
set_head (old_top, (size + old_size) | PREV_INUSE);

else if (contiguous (av) && old_size && brk < old_end)
/* Oops!  Someone else killed our space..  Can't touch anything.  */
malloc_printerr ("break adjusted to free malloc space");

/*
Otherwise, make adjustments:

* If the first time through or noncontiguous, we need to call sbrk
just to find out where the end of memory lies.

* We need to ensure that all returned chunks from malloc will meet
MALLOC_ALIGNMENT

* If there was an intervening foreign sbrk, we need to adjust sbrk
request size to account for fact that we will not be able to
combine new space with existing space in old_top.

* Almost all systems internally allocate whole pages at a time, in
which case we might as well use the whole last page of request.
So we allocate enough more memory to hit a page boundary now,
which in turn causes future contiguous calls to page-align.
*/

else
{
front_misalign = 0;
end_misalign = 0;
correction = 0;
aligned_brk = brk;

/* handle contiguous cases */
if (contiguous (av))
{
/* Count foreign sbrk as system_mem.  */
if (old_size)
av->system_mem += brk - old_end;

/* Guarantee alignment of first new chunk made from this space */

front_misalign = (INTERNAL_SIZE_T) chunk2mem (brk) & MALLOC_ALIGN_MASK;
if (front_misalign > 0)
{
/*
Skip over some bytes to arrive at an aligned position.
We don't need to specially mark these wasted front bytes.
They will never be accessed anyway because
prev_inuse of av->top (and any chunk created from its start)
is always true after initialization.
*/

correction = MALLOC_ALIGNMENT - front_misalign;
aligned_brk += correction;
}

/*
If this isn't adjacent to existing space, then we will not
be able to merge with old_top space, so must add to 2nd request.
*/

correction += old_size;

/* Extend the end address to hit a page boundary */
end_misalign = (INTERNAL_SIZE_T) (brk + size + correction);
correction += (ALIGN_UP (end_misalign, pagesize)) - end_misalign;

assert (correction >= 0);
snd_brk = (char *) (MORECORE (correction));

/*
If can't allocate correction, try to at least find out current
brk.  It might be enough to proceed without failing.

Note that if second sbrk did NOT fail, we assume that space
is contiguous with first sbrk. This is a safe assumption unless
program is multithreaded but doesn't use locks and a foreign sbrk
occurred between our first and second calls.
*/

if (snd_brk == (char *) (MORECORE_FAILURE))
{
correction = 0;
snd_brk = (char *) (MORECORE (0));
}
else
madvise_thp (snd_brk, correction);
}

/* handle non-contiguous cases */
else
{
if (MALLOC_ALIGNMENT == CHUNK_HDR_SZ)
/* MORECORE/mmap must correctly align */
assert (((unsigned long) chunk2mem (brk) & MALLOC_ALIGN_MASK) == 0);
else
{
front_misalign = (INTERNAL_SIZE_T) chunk2mem (brk) & MALLOC_ALIGN_MASK;
if (front_misalign > 0)
{
/*
Skip over some bytes to arrive at an aligned position.
We don't need to specially mark these wasted front bytes.
They will never be accessed anyway because
prev_inuse of av->top (and any chunk created from its start)
is always true after initialization.
*/

aligned_brk += MALLOC_ALIGNMENT - front_misalign;
}
}

/* Find out current end of memory */
if (snd_brk == (char *) (MORECORE_FAILURE))
{
snd_brk = (char *) (MORECORE (0));
}
}

/* Adjust top based on results of second sbrk */
if (snd_brk != (char *) (MORECORE_FAILURE))
{
av->top = (mchunkptr) aligned_brk;
set_head (av->top, (snd_brk - aligned_brk + correction) | PREV_INUSE);
av->system_mem += correction;

/*
If not the first time through, we either have a
gap due to foreign sbrk or a non-contiguous region.  Insert a
double fencepost at old_top to prevent consolidation with space
we don't own. These fenceposts are artificial chunks that are
marked as inuse and are in any case too small to use.  We need
two to make sizes and alignments work out.
*/

if (old_size != 0)
{
/*
Shrink old_top to insert fenceposts, keeping size a
multiple of MALLOC_ALIGNMENT. We know there is at least
enough space in old_top to do this.
*/
old_size = (old_size - 2 * CHUNK_HDR_SZ) & ~MALLOC_ALIGN_MASK;
set_head (old_top, old_size | PREV_INUSE);

/*
Note that the following assignments completely overwrite
old_top when old_size was previously MINSIZE.  This is
intentional. We need the fencepost, even if old_top otherwise gets
lost.
*/
set_head (chunk_at_offset (old_top, old_size),
CHUNK_HDR_SZ | PREV_INUSE);
set_head (chunk_at_offset (old_top,
old_size + CHUNK_HDR_SZ),
CHUNK_HDR_SZ | PREV_INUSE);

/* If possible, release the rest. */
if (old_size >= MINSIZE)
{
_int_free (av, old_top, 1);
}
}
}
}
}
} /* if (av !=  &main_arena) */
```
</details>

### рд╕рд┐рд╕рдореИрд▓реЛрдХ рдЕрдВрддрд┐рдо

рдЖрд░реЗрдирд╛ рд╕реВрдЪрдирд╛ рдХреЛ рдЕрдкрдбреЗрдЯ рдХрд░рдХреЗ рдЖрд╡рдВрдЯрди рд╕рдорд╛рдкреНрддрд┐
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2921C3-L2943C12

if ((unsigned long) av->system_mem > (unsigned long) (av->max_system_mem))
av->max_system_mem = av->system_mem;
check_malloc_state (av);

/* finally, do the allocation */
p = av->top;
size = chunksize (p);

/* check that one of the above allocation paths succeeded */
if ((unsigned long) (size) >= (unsigned long) (nb + MINSIZE))
{
remainder_size = size - nb;
remainder = chunk_at_offset (p, nb);
av->top = remainder;
set_head (p, nb | PREV_INUSE | (av != &main_arena ? NON_MAIN_ARENA : 0));
set_head (remainder, remainder_size | PREV_INUSE);
check_malloced_chunk (av, p, nb);
return chunk2mem (p);
}

/* catch all failure paths */
__set_errno (ENOMEM);
return 0;
```
## sysmalloc\_mmap

<details>

<summary>sysmalloc_mmap рдХреЛрдб</summary>
```c
// From https://github.com/bminor/glibc/blob/f942a732d37a96217ef828116ebe64a644db18d7/malloc/malloc.c#L2392C1-L2481C2

static void *
sysmalloc_mmap (INTERNAL_SIZE_T nb, size_t pagesize, int extra_flags, mstate av)
{
long int size;

/*
Round up size to nearest page.  For mmapped chunks, the overhead is one
SIZE_SZ unit larger than for normal chunks, because there is no
following chunk whose prev_size field could be used.

See the front_misalign handling below, for glibc there is no need for
further alignments unless we have have high alignment.
*/
if (MALLOC_ALIGNMENT == CHUNK_HDR_SZ)
size = ALIGN_UP (nb + SIZE_SZ, pagesize);
else
size = ALIGN_UP (nb + SIZE_SZ + MALLOC_ALIGN_MASK, pagesize);

/* Don't try if size wraps around 0.  */
if ((unsigned long) (size) <= (unsigned long) (nb))
return MAP_FAILED;

char *mm = (char *) MMAP (0, size,
mtag_mmap_flags | PROT_READ | PROT_WRITE,
extra_flags);
if (mm == MAP_FAILED)
return mm;

#ifdef MAP_HUGETLB
if (!(extra_flags & MAP_HUGETLB))
madvise_thp (mm, size);
#endif

__set_vma_name (mm, size, " glibc: malloc");

/*
The offset to the start of the mmapped region is stored in the prev_size
field of the chunk.  This allows us to adjust returned start address to
meet alignment requirements here and in memalign(), and still be able to
compute proper address argument for later munmap in free() and realloc().
*/

INTERNAL_SIZE_T front_misalign; /* unusable bytes at front of new space */

if (MALLOC_ALIGNMENT == CHUNK_HDR_SZ)
{
/* For glibc, chunk2mem increases the address by CHUNK_HDR_SZ and
MALLOC_ALIGN_MASK is CHUNK_HDR_SZ-1.  Each mmap'ed area is page
aligned and therefore definitely MALLOC_ALIGN_MASK-aligned.  */
assert (((INTERNAL_SIZE_T) chunk2mem (mm) & MALLOC_ALIGN_MASK) == 0);
front_misalign = 0;
}
else
front_misalign = (INTERNAL_SIZE_T) chunk2mem (mm) & MALLOC_ALIGN_MASK;

mchunkptr p;                    /* the allocated/returned chunk */

if (front_misalign > 0)
{
ptrdiff_t correction = MALLOC_ALIGNMENT - front_misalign;
p = (mchunkptr) (mm + correction);
set_prev_size (p, correction);
set_head (p, (size - correction) | IS_MMAPPED);
}
else
{
p = (mchunkptr) mm;
set_prev_size (p, 0);
set_head (p, size | IS_MMAPPED);
}

/* update statistics */
int new = atomic_fetch_add_relaxed (&mp_.n_mmaps, 1) + 1;
atomic_max (&mp_.max_n_mmaps, new);

unsigned long sum;
sum = atomic_fetch_add_relaxed (&mp_.mmapped_mem, size) + size;
atomic_max (&mp_.max_mmapped_mem, sum);

check_chunk (av, p);

return chunk2mem (p);
}
```
</details>

{% hint style="success" %}
рд╕реАрдЦреЗрдВ рдФрд░ рдкреНрд░реИрдХреНрдЯрд┐рд╕ рдХрд░реЗрдВ AWS рд╣реИрдХрд┐рдВрдЧ: <img src="/.gitbook/assets/arte.png" alt="" data-size="line">[**HackTricks рдкреНрд░рд╢рд┐рдХреНрд╖рдг AWS рд░реЗрдб рдЯреАрдо рдПрдХреНрд╕рдкрд░реНрдЯ (ARTE)**](https://training.hacktricks.xyz/courses/arte)<img src="/.gitbook/assets/arte.png" alt="" data-size="line">\
рд╕реАрдЦреЗрдВ рдФрд░ рдкреНрд░реИрдХреНрдЯрд┐рд╕ рдХрд░реЗрдВ GCP рд╣реИрдХрд┐рдВрдЧ: <img src="/.gitbook/assets/grte.png" alt="" data-size="line">[**HackTricks рдкреНрд░рд╢рд┐рдХреНрд╖рдг GCP рд░реЗрдб рдЯреАрдо рдПрдХреНрд╕рдкрд░реНрдЯ (GRTE)**<img src="/.gitbook/assets/grte.png" alt="" data-size="line">](https://training.hacktricks.xyz/courses/grte)

<details>

<summary>рд╣реИрдХрдЯреНрд░рд┐рдХреНрд╕ рдХрд╛ рд╕рдорд░реНрдерди рдХрд░реЗрдВ</summary>

* [**рд╕рдмреНрд╕рдХреНрд░рд┐рдкреНрд╢рди рдкреНрд▓рд╛рди**](https://github.com/sponsors/carlospolop) рдХреА рдЬрд╛рдБрдЪ рдХрд░реЗрдВ!
* **рд╢рд╛рдорд┐рд▓ рд╣реЛрдВ** ЁЯТм [**рдбрд┐рд╕реНрдХреЙрд░реНрдб рдЧреНрд░реБрдк**](https://discord.gg/hRep4RUj7f) рдпрд╛ [**рдЯреЗрд▓реАрдЧреНрд░рд╛рдо рдЧреНрд░реБрдк**](https://t.me/peass) рдпрд╛ **рдЯреНрд╡рд┐рдЯрд░** ЁЯРж рдкрд░ рд╣рдореЗрдВ **рдлреЙрд▓реЛ** рдХрд░реЗрдВ [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)**.**
* **рд╣реИрдХрд┐рдВрдЧ рдЯреНрд░рд┐рдХреНрд╕ рд╕рд╛рдЭрд╛ рдХрд░реЗрдВ PRs рд╕рдмрдорд┐рдЯ рдХрд░рдХреЗ** [**HackTricks**](https://github.com/carlospolop/hacktricks) рдФрд░ [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github рд░реЗрдкреЛ рдореЗрдВред

</details>
{% endhint %}
